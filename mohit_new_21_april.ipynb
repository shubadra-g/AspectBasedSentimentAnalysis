{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import re\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from functools import reduce\n",
    "from nltk.stem import PorterStemmer#, LancesterStemmer\n",
    "from collections import Counter \n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost',port=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_set = {'i', 'shan', 'just', 'how', 'each', 'out', 'themselves', 'their', 'before', 'were', 'very', 'as', 'further', 'his', 'a', 'once', 'youve', 'y', 'is', 'shouldve', 'youll', 'on', 'd', 'm', 'under', 'haven', 'which', 'only', 'them', 'was', 'by', 'needn', 'whom', 'that', 'when', 's', 'isn', 'its', 'wasn', 'in', 'we', 'theirs', 'those', 'this', 'having', 'and', 'ain', 'most', 'up', 'off', 'being', 'aren', 'shouldn', 'ourselves', 'from', 'down', 'herself', 'her', 'you', 'are', 'its', 'who', 'the', 'here', 'where', 'your', 'youd', 'she', 'didn', 'weren', 'about', 'has', 'our', 'an', 'yourselves', 'or', 'hasn', 'again', 'while', 'does', 'him', 'shes', 'above', 'below', 'itself', 'to', 'through', 'will', 'couldn', 'hers', 'they', 'doing', 'because', 'he', 'what', 'such', 'youre', 'nor', 'too', 'should', 'ours', 'then', 'himself', 'all', 'of', 'mightn', 'between', 'now', 'against', 'some', 'with', 'until', 'am', 'other', 'at', 'can', 'over', 'mustn', 'wouldn', 'do', 'for', 'after', 'hadn', 'me', 'been', 'same', 'doesn', 'my', 'these', 'll', 'did', 'had', 'it', 'so', 'ma', 'during', 'than', 'o', 'yourself', 'own', 'have', 're', 've', 'be', 'why', 't', 'there', 'more', 'won', 'yours', 'few', 'into', 'thatll', 'any', 'myself', 'both', 'don', 'if'}\n",
    "\n",
    "important_pos_tag_list_at = ['CC', 'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'FW','IN', 'CD']\n",
    "important_pos_tag_list_sent = ['CC', 'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "stemmer_tag_list = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "sentence_weights = []\n",
    "data_X, data_Y, data_X1 = [], [], []\n",
    "porter_stemmer = PorterStemmer()\n",
    "word_net_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'project_2_train/' + 'dara 1_train.csv'\n",
    "file_obj = open(file_name, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1650_0', ['macbook', 'come', 'freesecuritysoftware', 'protect', 'virus', 'intrusive', 'thing', 'downloads', 'internet', 'surfing', 'email'], ['securitysoftware'], [[17, 33]], '1\\n', []]\n",
      "---------------------------------\n",
      "1650_0,With the macbook pro it comes with freesecuritysoftware to protect it from viruses and other intrusive things from downloads and internet surfing or emails.,securitysoftware,39--55,1\n",
      "\n",
      "['1650_0', ['macbook', 'come', 'freesecuritysoftware', 'protect', 'virus', 'intrusive', 'thing', 'downloads', 'internet', 'surfing', 'email'], ['securitysoftware'], [[17, 33]], '1\\n', []]\n",
      "[['macbook', 0], ['come', 0], ['freesecuritysoftware', 0], ['protect', 0], ['virus', 0], ['intrusive', 0], ['thing', 0], ['downloads', 0], ['internet', 0], ['surfing', 0], ['email', 0]]\n",
      "['2261_1', ['apple', 'application', 'exiphoto', 'fun', 'easy', 'really', 'cool', 'use', 'competition'], ['iphoto'], [[20, 26]], '1\\n', []]\n",
      "---------------------------------\n",
      "2261_1,The Apple applications (ex.iPhoto) are fun[comma] easy[comma] and really cool to use (unlike the competition)!,iPhoto,27--33,1\n",
      "\n",
      "['2261_1', ['apple', 'application', 'exiphoto', 'fun', 'easy', 'really', 'cool', 'use', 'competition'], ['iphoto'], [[20, 26]], '1\\n', []]\n",
      "[['apple', 0], ['application', 0], ['exiphoto', 0], ['fun', 0], ['easy', 0], ['really', 0], ['cool', 0], ['use', 0], ['competition', 0]]\n"
     ]
    }
   ],
   "source": [
    "for line_index, line in enumerate(file_obj):\n",
    "    if line_index != 0:\n",
    "        ''' Splitting the columns based on comma - since it is csv'''\n",
    "        columns = line.split(',')\n",
    "\n",
    "        ''' The comma in the actual sentence was represented as [commma] because of csv format, replace that '''\n",
    "        columns[1] = columns[1].replace('[comma]', ',')\n",
    "        columns[2] = columns[2].replace('[comma]', ',')\n",
    "\n",
    "        ''' Some aspect terms are mic in the sentence where computer/mic is present '''\n",
    "        columns[1] = columns[1].replace('-', ' ')\n",
    "        columns[2] = columns[2].replace('-', ' ')\n",
    "        columns[1] = columns[1].replace('/', ' ')\n",
    "        columns[2] = columns[2].replace('/', ' ')\n",
    "        \n",
    "        ''' POS Tagging the sentence'''\n",
    "        pos_tagged_sentence = nlp.pos_tag(columns[1])\n",
    "        pos_tagged_aspect_term = nlp.pos_tag(columns[2])\n",
    "#         porter_stemmer\n",
    "\n",
    "    \n",
    "        for word_index, word in enumerate(pos_tagged_sentence):\n",
    "            pos_tagged_sentence[word_index] = list(word)\n",
    "            '''Remove special characters'''\n",
    "            if word[0] not in columns[2]:\n",
    "                pos_tagged_sentence[word_index][0] = re.sub('[^0-9a-zA-Z]+', '', word[0]).lower()\n",
    "                ''' didn't is tokenized as did n't hence following code '''\n",
    "                if pos_tagged_sentence[word_index][0] == 'nt':\n",
    "                    pos_tagged_sentence[word_index][0] = 'not'\n",
    "                \n",
    "        '''Remove empty string tokens'''\n",
    "        pos_tagged_sentence = [x for x in pos_tagged_sentence if x[0].strip() != '']\n",
    "        pos_tagged_aspect_term = [x for x in pos_tagged_aspect_term if x[0].strip() != '']\n",
    "        \n",
    "        for word_tag_index, word_tag in enumerate(pos_tagged_sentence):\n",
    "            pos_tagged_sentence[word_tag_index] = list(word_tag)\n",
    "            if word_tag[1] in stemmer_tag_list and word_tag[0] not in columns[2]:\n",
    "#                 pos_tagged_sentence[word_tag_index][0] = porter_stemmer.stem(word_tag[0])                \n",
    "#                 pos_tagged_sentence[word_tag_index][0] = porter_stemmer.stem(word_net_lemmatizer.lemmatize(spell(word_tag[0]), get_wordnet_pos(word_tag[1])))\n",
    "                pos_tagged_sentence[word_tag_index][0] = word_net_lemmatizer.lemmatize(word_tag[0], get_wordnet_pos(word_tag[1]))\n",
    "        \n",
    "        \n",
    "        ''' For aspect term: Removing the words whose POS tag is not important'''\n",
    "        columns[2] = []\n",
    "        for aspect_word_tag in pos_tagged_aspect_term:\n",
    "            if aspect_word_tag[1] in important_pos_tag_list_at:\n",
    "                columns[2].append(aspect_word_tag[0].strip())\n",
    "        \n",
    "        '''If it removes every word in the aspect term then print'''\n",
    "        if len(columns[2]) == 0:\n",
    "            print(pos_tagged_aspect_term)\n",
    "\n",
    "        ''' For sentence: Removing the words whose POS tag is not important'''\n",
    "        columns[1] = []\n",
    "        for word_tag in pos_tagged_sentence:\n",
    "            if word_tag[1] in important_pos_tag_list_sent or word_tag[0] in columns[2]:\n",
    "                columns[1].append(word_tag[0].strip())\n",
    "    \n",
    "        '''If it removes every word in the sentence then print'''\n",
    "        if len(columns[1]) == 0:\n",
    "            print(pos_tagged_sentence)\n",
    "        \n",
    "        for word_index, word in enumerate(columns[1]):\n",
    "            '''Remove special characters'''\n",
    "            columns[1][word_index] = re.sub('[^0-9a-zA-Z]+', '', word).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[1][word_index] == 'nt':\n",
    "                columns[1][word_index] = 'not'\n",
    "        \n",
    "        for word_index, word in enumerate(columns[2]):\n",
    "            '''Remove special characters'''\n",
    "            columns[2][word_index] = re.sub('[^0-9a-zA-Z]+', '', word).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[2][word_index] == 'nt':\n",
    "                columns[2][word_index] = 'not'\n",
    "        \n",
    "        '''Remove empty string tokens'''\n",
    "        columns[1] = [x.strip() for x in columns[1] if x.strip() != '']\n",
    "        columns[2] = [x.strip() for x in columns[2] if x.strip() != '']\n",
    "        \n",
    "        ''' Remove stop words '''\n",
    "        columns[2] = [word for word in columns[2] if word not in stopwords_set]\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        \n",
    "#         spell\n",
    "        \n",
    "        if len(columns[2]) == 0:\n",
    "            print(pos_tagged_aspect_term)\n",
    "    \n",
    "        columns[1] = ' '.join(columns[1])\n",
    "        columns[2] = ' '.join(columns[2])\n",
    "\n",
    "        ''' The aspect term location given is not proper - hence extracting the location by ourselves '''\n",
    "        columns[3] = []\n",
    "        \n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "            \n",
    "        ''' Tokenizing the words again '''\n",
    "        columns[1] = columns[1].split(' ')\n",
    "        columns[2] = columns[2].split(' ')\n",
    "        \n",
    "        ''' New column to specify the aspect term location in the list '''\n",
    "        columns.append([])\n",
    "        \n",
    "        ''' for multiple positions of aspect term '''\n",
    "        for aspect_term_loc in columns[3]:\n",
    "            if aspect_term_loc[0] == 0: #start index of the 1st occurance of the aspect term\n",
    "                columns[5].append([0])\n",
    "            else:\n",
    "                temp_len = aspect_term_loc[0] # assign the start position of the aspect term\n",
    "                for word_index, word in enumerate(columns[1]):\n",
    "                    temp_len -= (len(word) + 1) # Counting the words till the position - +1 for blank space\n",
    "                    if temp_len == 0: # Reached the aspect word\n",
    "                        columns[5].append([word_index+1])\n",
    "                        break\n",
    "            for k in range(len(columns[2])-1): # if multiple words in the aspect term - tag the following words in the sentence\n",
    "                columns[5][-1].append(columns[5][-1][-1]+1)\n",
    "\n",
    "        ''' if not found the aspect term location in the list then do '''\n",
    "        if len(columns[5]) < 1:\n",
    "            print(columns)\n",
    "            print('---------------------------------')\n",
    "#             continue\n",
    "        \n",
    "        word_wt_list = [[word, 0] for word in columns[1]]\n",
    "        \n",
    "        ''' run only if aspect term found, otherwise the weights are zero '''\n",
    "        if len(columns[5]) >= 1:\n",
    "        \n",
    "            ''' Assigning weights to every word based on the distance from the aspect term '''\n",
    "            for j, elem in enumerate(columns[5]):\n",
    "                for k, word in enumerate(columns[1]):\n",
    "                    if k < elem[0]: # Word left to the aspect term\n",
    "                        dist = abs(elem[0] - k)\n",
    "                        if word_wt_list[k][1] < 1/dist:\n",
    "                            word_wt_list[k][1] = 1/dist\n",
    "                    elif k > elem[-1]: # word right to the aspect term : Aspect term can have multiple words\n",
    "                        dist = abs(k - elem[-1])\n",
    "                        if word_wt_list[k][1] < 1/dist:\n",
    "                            word_wt_list[k][1] = 1/dist\n",
    "                for aspect_word_loc in elem:\n",
    "                    word_wt_list[aspect_word_loc][1] = 1.5\n",
    "\n",
    "        ''' For duplicate words - if it's aspect term - add the weights, if not, then take the weight which is greater, make the other weight as zero '''\n",
    "        for j, word_1 in enumerate(word_wt_list):\n",
    "            for k, word_2 in enumerate(word_wt_list[j+1:]):\n",
    "                if word_1[0] == word_2[0]:\n",
    "                    if word_1[1] == 1.5 and word_2[1] == 1.5:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_1[1] += 1.5\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] < word_2[1]:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_1[1] = word_2[1]\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] > word_2[1]:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] == word_2[1]:\n",
    "                        word_2[1] = 0\n",
    "    \n",
    "        if len(columns[5]) >= 1:\n",
    "            ''' For removing duplicate words - they have weight zero now'''\n",
    "            word_wt_list = [elem for elem in word_wt_list if elem[1] != 0]\n",
    "        else: \n",
    "            print(line)\n",
    "            print(columns)\n",
    "            print(word_wt_list)\n",
    "\n",
    "        if len(columns[2]) > 1 and len(columns[5]) > 1:\n",
    "#             print(line)\n",
    "#             print(columns)\n",
    "#             print(word_wt_list)\n",
    "            pass\n",
    "\n",
    "        if columns[0] == '2911_0':\n",
    "#             print(line)\n",
    "#             print(word_wt_list)\n",
    "            pass\n",
    "\n",
    "        '''Removing new line character from columns[4] -  the class column'''\n",
    "        columns[4] = columns[4].rstrip('\\n')\n",
    "        \n",
    "        data_X.append(columns[1])\n",
    "        data_X1.append(' '.join(columns[1]))\n",
    "\n",
    "        data_Y.append(columns[4])\n",
    "        sentence_weights.append(word_wt_list)\n",
    "        \n",
    "    if line_index == 10:\n",
    "#         break\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2429\n"
     ]
    }
   ],
   "source": [
    "'''Building vocabulary and counting word occurances'''\n",
    "\n",
    "X_reduced = reduce(lambda x1, x2: x1 + x2, data_X)\n",
    "vocab = list(set(X_reduced))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_v = np.zeros_like(vocab, dtype = np.float_)\n",
    "weight_x = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = open('weightxy_data1_spell_lemm.csv', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Writing column labels for pandas dataframe'''\n",
    "pf.write(','.join(vocab))\n",
    "pf.write(',class_')\n",
    "pf.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['macbook', 0], ['come', 0], ['freesecuritysoftware', 0], ['protect', 0], ['virus', 0], ['intrusive', 0], ['thing', 0], ['downloads', 0], ['internet', 0], ['surfing', 0], ['email', 0]]\n",
      "11\n",
      "[]\n",
      "ksdjnfsdnfk\n",
      "[['apple', 0], ['application', 0], ['exiphoto', 0], ['fun', 0], ['easy', 0], ['really', 0], ['cool', 0], ['use', 0], ['competition', 0]]\n",
      "9\n",
      "[]\n",
      "ksdjnfsdnfk\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentence_weights):\n",
    "    for word in sentence:\n",
    "        v_index = vocab.index(word[0])\n",
    "        weight_v[v_index] = word[1]\n",
    "\n",
    "    weight_x.append(weight_v)\n",
    "    if len(sentence) != weight_v[np.where(weight_v > 0)].shape[0]:\n",
    "        print(sentence)\n",
    "        print(len(sentence))\n",
    "        print(weight_v[np.where(weight_v > 0)])\n",
    "\n",
    "    ''' code to write the data to file '''\n",
    "    temp = [str(j) for j in weight_v.tolist()]\n",
    "    pf.write(','.join(temp))\n",
    "    if len(weight_v[np.where(weight_v > 0)]) == 0 :\n",
    "        pf.write(','+str(0))\n",
    "        print('ksdjnfsdnfk')\n",
    "    else:\n",
    "        pf.write(','+str(data_Y[i]))\n",
    "    pf.write('\\n')\n",
    "    weight_v = np.zeros_like(weight_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2313, 2429)\n"
     ]
    }
   ],
   "source": [
    "weight_x = np.array(weight_x)\n",
    "\n",
    "print(weight_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
