{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sense\n",
      "(3602, 3612)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Apr  6 16:09:25 2018\n",
    "\n",
    "@author: Shubadra\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter \n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "# def get_pos( word ):\n",
    "#     w_synsets = wordnet.synsets(word)\n",
    "\n",
    "#     pos_counts = Counter()\n",
    "#     pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "#     pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "#     pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "#     pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "#     most_common_pos_list = pos_counts.most_common(3)\n",
    "#     return most_common_pos_list[0][0]\n",
    "\n",
    "find = lambda searchList, elem: [[i for i, x in enumerate(searchList) if x == e] for e in elem]\n",
    "fname = 'project_2_train/' + 'data 2_train.csv'\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_set = {'i', 'shan', 'just', 'how', 'each', 'out', 'themselves', 'their', 'before', 'were', 'very', 'as', 'further', 'his', 'a', 'once', 'youve', 'y', 'is', 'shouldve', 'youll', 'on', 'd', 'm', 'under', 'haven', 'which', 'only', 'them', 'was', 'by', 'needn', 'whom', 'that', 'when', 's', 'isn', 'its', 'no', 'wasn', 'in', 'we', 'theirs', 'those', 'this', 'having', 'and', 'ain', 'most', 'up', 'off', 'being', 'aren', 'shouldn', 'ourselves', 'from', 'down', 'herself', 'her', 'you', 'are', 'its', 'who', 'the', 'here', 'where', 'your', 'youd', 'she', 'didn', 'weren', 'about', 'has', 'our', 'an', 'yourselves', 'or', 'hasn', 'again', 'while', 'does', 'him', 'shes', 'above', 'below', 'itself', 'to', 'through', 'will', 'couldn', 'hers', 'they', 'doing', 'because', 'he', 'what', 'such', 'youre', 'nor', 'too', 'should', 'ours', 'then', 'himself', 'all', 'of', 'mightn', 'between', 'now', 'against', 'some', 'with', 'until', 'am', 'other', 'at', 'can', 'over', 'mustn', 'wouldn', 'do', 'for', 'after', 'hadn', 'me', 'been', 'same', 'doesn', 'my', 'these', 'll', 'did', 'had', 'it', 'so', 'ma', 'during', 'than', 'o', 'yourself', 'own', 'have', 're', 've', 'be', 'why', 't', 'there', 'more', 'won', 'yours', 'few', 'into', 'thatll', 'any', 'myself', 'both', 'don', 'if'}\n",
    "# print(stopwords_set)\n",
    "# stopwords_set.remove('but')\n",
    "# stopwords_set.remove('not')\n",
    "\n",
    "sentence_weights = []\n",
    "X, Y, X1 = [], [], []\n",
    "\n",
    "f = open(fname, 'r')\n",
    "for i, line in enumerate(f):\n",
    "    if i != 0:\n",
    "        ''' Splitting the columns based on comma - since it is csv'''\n",
    "        columns = line.split(',')\n",
    "\n",
    "        ''' The comma in the actual sentence was represented as [commma] because of csv format, replace that '''\n",
    "        columns[1] = columns[1].replace('[comma]', ',')\n",
    "        columns[2] = columns[2].replace('[comma]', ',')\n",
    "\n",
    "        ''' Some aspect terms are mic in the sentence where computer/mic is present '''\n",
    "        columns[1] = columns[1].replace('-', ' ')\n",
    "        columns[2] = columns[2].replace('-', ' ')\n",
    "        columns[1] = columns[1].replace('/', ' ')\n",
    "        columns[2] = columns[2].replace('/', ' ')\n",
    "\n",
    "        ''' NOt used anymore'''\n",
    "        tokenizer = RegexpTokenizer(r'\\w+') # doesn't work if the special char is in the token\n",
    "        # columns[2] = tokenizer.tokenize(columns[2].lower())\n",
    "\n",
    "        '''Tokenize the words'''\n",
    "        columns[1] = word_tokenize(columns[1])\n",
    "        columns[2] = word_tokenize(columns[2])\n",
    "\n",
    "        for j, elem in enumerate(columns[1]):\n",
    "            '''Remove special characters'''\n",
    "            columns[1][j] = re.sub('[^0-9a-zA-Z]+', '', elem).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[1][j] == 'nt':\n",
    "                columns[1][j] = 'not'\n",
    "\n",
    "        for j, elem in enumerate(columns[2]):\n",
    "            '''Remove special characters'''\n",
    "            columns[2][j] = re.sub('[^0-9a-zA-Z]+', '', elem).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[2][j] == 'nt':\n",
    "                columns[2][j] = 'not'\n",
    "\n",
    "        '''Remove empty string tokens'''\n",
    "        columns[1] = [x.strip() for x in columns[1] if x.strip() != '']\n",
    "        columns[2] = [x.strip() for x in columns[2] if x.strip() != '']\n",
    "\n",
    "        ''' Remove stop words '''\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        columns[2] = [word for word in columns[2] if word not in stopwords_set]\n",
    "        \n",
    "#         '''Lemmatizing followed by Stemming'''\n",
    "#         ps = PorterStemmer()\n",
    "#         wnl = WordNetLemmatizer()\n",
    "#         for word_index, word in enumerate(columns[1]):\n",
    "#             columns[1][word_index] = ps.stem(word)\n",
    "#         for aspect_term_index, aspect_term in enumerate(columns[2]):\n",
    "#             columns[1][aspect_term_index] = ps.stem(wnl.lemmatize(aspect_term))\n",
    " \n",
    "        columns[1] = ' '.join(columns[1])\n",
    "        columns[2] = ' '.join(columns[2])\n",
    "\n",
    "        ''' The aspect term location given is not proper - hence extracting the location by ourselves '''\n",
    "        columns[3] = []\n",
    "        ''' Finds the location of the aspect term in the sentence '''\n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "            ''' since some aspect terms are not surrounded by spaces, hence below code\n",
    "            NOT USED ANYMORE\n",
    "            '''\n",
    "            if m.start() != 0 and columns[1][m.start() - 1] != ' ':\n",
    "                # columns[1] = columns[1][:m.start()] + ' ' + columns[1][m.start():]\n",
    "                pass\n",
    "\n",
    "        ''' Because of adding space in above code, the position is messed up, hence redoing it. '''\n",
    "        columns[3] = []\n",
    "\n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "\n",
    "        ''' Tokenizing the words again '''\n",
    "        columns[1] = columns[1].split(' ')\n",
    "        columns[2] = columns[2].split(' ')\n",
    "        ''' New column to specify the aspect term location in the list '''\n",
    "        columns.append([])\n",
    "        # columns[5] = []\n",
    "        ''' for multiple positions of aspect term '''\n",
    "        for elem in columns[3]:\n",
    "            if elem[0] == 0: #start index of the 1st occurance of the aspect term\n",
    "                columns[5].append([0])\n",
    "            else:\n",
    "                temp_len = elem[0] # assign the start position of the aspect term\n",
    "                for j, tokens in enumerate(columns[1]):\n",
    "                    temp_len -= (len(tokens) + 1) # Counting the words till the position - +1 for blank space\n",
    "                    if temp_len == 0: # Reached the aspect word\n",
    "                        columns[5].append([j+1])\n",
    "                        break\n",
    "            for k in range(len(columns[2])-1): # if multiple words in the aspect term - tag the following words in the sentence\n",
    "                columns[5][-1].append(columns[5][-1][-1]+1)\n",
    "\n",
    "        ''' if not found the aspect term location in the list then do '''\n",
    "        if len(columns[5]) < 1:\n",
    "            continue\n",
    "\n",
    "            # pass\n",
    "\n",
    "        word_wt_list = [[elem, 0] for elem in columns[1]]\n",
    "        ''' Assigning weights to every word based on the distance from the aspect term '''\n",
    "        for j, elem in enumerate(columns[5]):\n",
    "            for k, word in enumerate(columns[1]):\n",
    "                if k < elem[0]: # Word left to the aspect term\n",
    "                    dist = abs(elem[0] - k)\n",
    "                    if word_wt_list[k][1] < 1/dist:\n",
    "                        word_wt_list[k][1] = 1/dist\n",
    "                elif k > elem[-1]: # word right to the aspect term : Aspect term can have multiple words\n",
    "                    dist = abs(k - elem[-1])\n",
    "                    if word_wt_list[k][1] < 1/dist:\n",
    "                        word_wt_list[k][1] = 1/dist\n",
    "            for aspect_word_loc in elem:\n",
    "                word_wt_list[aspect_word_loc][1] = 1.5\n",
    "\n",
    "        ''' For duplicate words - if it's aspect term - add the weights, if not, then take the weight which is greater, make the other weight as zero '''\n",
    "        for j, word_1 in enumerate(word_wt_list):\n",
    "            for k, word_2 in enumerate(word_wt_list[j+1:]):\n",
    "                if word_1[0] == word_2[0]:\n",
    "                    if word_1[1] == 1.5 and word_2[1] == 1.5:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_1[1] += 1.5\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] < word_2[1]:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_1[1] = word_2[1]\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] > word_2[1]:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] == word_2[1]:\n",
    "                        word_2[1] = 0\n",
    "\n",
    "\n",
    "        ''' For removing duplicate words - they have weight zero now'''\n",
    "        word_wt_list = [elem for elem in word_wt_list if elem[1] != 0]\n",
    "\n",
    "        if len(columns[2]) > 1 and len(columns[5]) > 1:\n",
    "            # print(line)\n",
    "            # print(columns)\n",
    "            # print(word_wt_list)\n",
    "            pass\n",
    "\n",
    "        if columns[0] == '2911_0':\n",
    "            # print(line)\n",
    "            # print(word_wt_list)\n",
    "            pass\n",
    "        if i == 100:\n",
    "            # break\n",
    "            pass\n",
    "\n",
    "        '''Removing new line character from columns[4] -  the polarity column'''\n",
    "        columns[4] = columns[4].rstrip('\\n')\n",
    "        X.append(columns[1])\n",
    "        X1.append(' '.join(columns[1]))\n",
    "\n",
    "        Y.append(columns[4])\n",
    "        sentence_weights.append(word_wt_list)\n",
    "\n",
    "\n",
    "'''Building vocabulary and counting word occurances'''\n",
    "X_reduced = reduce(lambda x1, x2: x1 + x2, X)\n",
    "vocab = list(set(X_reduced))\n",
    "print(vocab[0])\n",
    "weight_v = np.zeros_like(vocab, dtype = np.float_)\n",
    "weight_x = []\n",
    "pf = open('weightxy_data2.csv', 'w')\n",
    "\n",
    "pf.write(','.join(vocab))\n",
    "pf.write(',class_')\n",
    "pf.write('\\n')\n",
    "for i, sentence in enumerate(sentence_weights):\n",
    "    for word in sentence:\n",
    "        v_index = vocab.index(word[0])\n",
    "        weight_v[v_index] = word[1]\n",
    "\n",
    "    weight_x.append(weight_v)\n",
    "    if len(sentence) != weight_v[np.where(weight_v > 0)].shape[0]:\n",
    "        print(sentence)\n",
    "        print(len(sentence))\n",
    "        print(weight_v[np.where(weight_v > 0)])\n",
    "\n",
    "    ''' code to write the data to file '''\n",
    "    temp = [str(j) for j in weight_v.tolist()]\n",
    "    pf.write(','.join(temp))\n",
    "    pf.write(','+str(Y[i]))\n",
    "    pf.write('\\n')\n",
    "    weight_v = np.zeros_like(weight_v)\n",
    "\n",
    "pf.close()\n",
    "weight_x = np.array(weight_x)\n",
    "\n",
    "print(weight_x.shape)\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "# weight_X2 = np.reshape(weight_X, (len(Y) ,weight_X.shape[0]))\n",
    "# print(weight_X2.shape)\n",
    "# tokens = f.split(',') # columns\n",
    "# remove punctuations\n",
    "# table = str.maketrans('', '', string.punctuation)\n",
    "# print(table)\n",
    "# tokens = [w.translate(table) for w in tokens]\n",
    "# # remove tokens that are not alphbetic\n",
    "# tokens = [word for word in tokens if word.isalpha()]\n",
    "#\n",
    "# # remove stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tokens = [w for w in tokens if not w in stop_words]\n",
    "# # filter out short tokens\n",
    "# tokens = [word for word in tokens if len(word) > 1]\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3602, 3613)\n",
      "(3602, 3612)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load data set'''\n",
    "df1 = pd.read_csv(\"weightxy_data2.csv\")\n",
    "print(df1.shape)\n",
    "print(df1.iloc[:,:-1].shape)\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "(3602, 3612)\n",
      "(3602,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(df1.iloc[:,:-1], df1['class_'], test_size = 0.3, random_state =42)\n",
    "X, Y = df1.iloc[:,:-1], df1['class_']\n",
    "\n",
    "# svm_clf = svm.SVC(kernel = 'rbf')\n",
    "svm_clf = svm.LinearSVC(C = 0.1, random_state = 0) #loss = 'hinge') #, multi_class = 'crammer_singer')\n",
    "\n",
    "# svm_clf.fit(X, Y)\n",
    "print('success')\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([ 0.22421408,  0.23439264,  0.21877432,  0.21277785,  0.34834719,\n",
      "        0.21967864,  0.21164298,  0.20967221,  0.21877408,  0.22537494]), 'score_time': array([ 0.22360039,  0.        ,  0.        ,  0.01562786,  0.01404166,\n",
      "        0.01562929,  0.        ,  0.        ,  0.        ,  0.        ]), 'test_score': array([ 0.72099448,  0.72651934,  0.73480663,  0.66204986,  0.72222222,\n",
      "        0.70194986,  0.70194986,  0.67688022,  0.70473538,  0.67966574]), 'train_score': array([ 0.90432099,  0.90617284,  0.90401235,  0.91237272,  0.90746453,\n",
      "        0.90595128,  0.90718471,  0.91026827,  0.90595128,  0.90595128])}\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cross_validate(svm_clf, X, Y, cv = 10, scoring = 'accuracy')\n",
    "\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...,  1  0  0]\n",
      "success\n",
      "\n",
      "\n",
      "Model Accuracy is: 70.3220%\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.60      0.54       653\n",
      "          1       0.31      0.52      0.39       384\n",
      "         -1       0.90      0.76      0.82      2565\n",
      "\n",
      "avg / total       0.76      0.70      0.72      3602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Cross validation'''\n",
    "y_pred = cross_val_predict(svm_clf, X, Y, cv = 10) #, scoring = 'accuracy')\n",
    "\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "\n",
    "mod_accracy = accuracy_score(y_pred, Y)\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(y_pred, Y, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n",
      "success\n",
      "{'fit_time': array([ 5.25342131,  5.08987379,  5.27256608,  6.38939095,  5.57181549,\n",
      "        5.61872816,  5.44238305,  5.18801045,  5.29704666,  5.19146609]), 'score_time': array([ 0.03128791,  0.04699802,  0.04685092,  0.04311419,  0.04311299,\n",
      "        0.04683065,  0.05353093,  0.04688287,  0.03130293,  0.02844381]), 'test_score': array([ 0.67955801,  0.63535912,  0.64917127,  0.61218837,  0.68611111,\n",
      "        0.64066852,  0.63509749,  0.64623955,  0.61002786,  0.66852368]), 'train_score': array([ 0.77438272,  0.77561728,  0.76851852,  0.77198396,  0.76588526,\n",
      "        0.77489978,  0.77181622,  0.7613321 ,  0.76441566,  0.76688252])}\n",
      "[-1 -1  1 ..., -1 -1 -1]\n",
      "success\n",
      "\n",
      "\n",
      "Model Accuracy is: 64.8529%\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.48      0.42       632\n",
      "          1       0.28      0.45      0.34       387\n",
      "         -1       0.86      0.72      0.78      2583\n",
      "\n",
      "avg / total       0.71      0.65      0.67      3602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "adaBoost_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2)) #, n_estimators=2, learning_rate=1)\n",
    "scores = cross_validate(adaBoost_clf, X, Y, cv = 10)\n",
    "print(scores.keys())\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "print(scores)\n",
    "\n",
    "y_pred = cross_val_predict(adaBoost_clf, X, Y, cv = 10) #, scoring = 'accuracy')\n",
    "\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "\n",
    "mod_accracy = accuracy_score(y_pred, Y)\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(y_pred, Y, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n",
      "success\n",
      "{'fit_time': array([ 0.19217801,  0.0937593 ,  0.10939217,  0.10822964,  0.09376216,\n",
      "        0.10864186,  0.12501478,  0.11818719,  0.12501001,  0.11589599]), 'score_time': array([ 0.00501418,  0.01563025,  0.        ,  0.01562786,  0.01562476,\n",
      "        0.0156281 ,  0.        ,  0.        ,  0.        ,  0.        ]), 'test_score': array([ 0.70165746,  0.69060773,  0.66022099,  0.65373961,  0.67222222,\n",
      "        0.64345404,  0.65738162,  0.61002786,  0.66573816,  0.62674095]), 'train_score': array([ 0.84259259,  0.84753086,  0.84598765,  0.84757791,  0.84577421,\n",
      "        0.84736355,  0.8470552 ,  0.84828862,  0.84643848,  0.84304656])}\n",
      "[-1 -1  1 ...,  1  1  1]\n",
      "success\n",
      "\n",
      "\n",
      "Model Accuracy is: 65.8245%\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.55      0.49       638\n",
      "          1       0.29      0.40      0.33       452\n",
      "         -1       0.85      0.73      0.79      2512\n",
      "\n",
      "avg / total       0.71      0.66      0.68      3602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using MNB: \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb_clf = MultinomialNB()\n",
    "\n",
    "scores = cross_validate(mnb_clf, X, Y, cv = 10)\n",
    "print(scores.keys())\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "print(scores)\n",
    "\n",
    "y_pred = cross_val_predict(mnb_clf, X, Y, cv = 10) #, scoring = 'accuracy')\n",
    "\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "\n",
    "mod_accracy = accuracy_score(y_pred, Y)\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(y_pred, Y, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n",
      "success\n",
      "{'fit_time': array([ 0.1562264 ,  0.10944152,  0.10939145,  0.09377956,  0.10448217,\n",
      "        0.10942316,  0.10942674,  0.1093874 ,  0.11043811,  0.10944247]), 'score_time': array([ 0.        ,  0.01557374,  0.        ,  0.01558328,  0.01559043,\n",
      "        0.        ,  0.01558948,  0.        ,  0.00601602,  0.01557326]), 'test_score': array([ 0.59944751,  0.59944751,  0.59944751,  0.60110803,  0.6       ,\n",
      "        0.60167131,  0.60167131,  0.60167131,  0.60167131,  0.60167131]), 'train_score': array([ 0.60092593,  0.60216049,  0.60092593,  0.60074051,  0.60086366,\n",
      "        0.60160345,  0.60098674,  0.60067838,  0.60067838,  0.60160345])}\n",
      "[1 1 1 ..., 1 1 1]\n",
      "success\n",
      "\n",
      "\n",
      "Model Accuracy is: 60.0777%\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          1       0.00      0.00      0.00         0\n",
      "         -1       1.00      0.60      0.75      3602\n",
      "\n",
      "avg / total       1.00      0.60      0.75      3602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubadra\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomForest_clf = RandomForestClassifier(max_depth=2, random_state=4)\n",
    "\n",
    "scores = cross_validate(randomForest_clf, X, Y, cv = 10)\n",
    "print(scores.keys())\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "print(scores)\n",
    "\n",
    "y_pred = cross_val_predict(randomForest_clf, X, Y, cv = 10) #, scoring = 'accuracy')\n",
    "\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "\n",
    "mod_accracy = accuracy_score(y_pred, Y)\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(y_pred, Y, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  1 ...,  1  1  0]\n",
      "success\n",
      "\n",
      "\n",
      "Model Accuracy is: 65.0194%\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.52      0.50       741\n",
      "          1       0.38      0.38      0.38       626\n",
      "         -1       0.79      0.77      0.78      2235\n",
      "\n",
      "avg / total       0.66      0.65      0.65      3602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
    "X, Y = df1.iloc[:,:-1], df1['class_']\n",
    "mlp.fit(X, Y)\n",
    "\n",
    "y_pred = cross_val_predict(mlp, X, Y, cv = 10)\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "\n",
    "mod_accracy = accuracy_score(y_pred, Y)\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(y_pred, Y, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3602/3602 [==============================] - 2s 469us/step - loss: -0.2473 - acc: 0.4345\n",
      "Epoch 2/150\n",
      "3602/3602 [==============================] - 1s 308us/step - loss: -2.2139 - acc: 0.6602\n",
      "Epoch 3/150\n",
      "3602/3602 [==============================] - 1s 318us/step - loss: -2.7887 - acc: 0.7016\n",
      "Epoch 4/150\n",
      "3602/3602 [==============================] - 1s 313us/step - loss: -3.0254 - acc: 0.7215\n",
      "Epoch 5/150\n",
      "3602/3602 [==============================] - 1s 313us/step - loss: -3.1434 - acc: 0.7324\n",
      "Epoch 6/150\n",
      "3602/3602 [==============================] - 1s 312us/step - loss: -3.2046 - acc: 0.7404\n",
      "Epoch 7/150\n",
      "3602/3602 [==============================] - 1s 314us/step - loss: -3.2459 - acc: 0.7465\n",
      "Epoch 8/150\n",
      "3602/3602 [==============================] - 1s 339us/step - loss: -3.2690 - acc: 0.7471\n",
      "Epoch 9/150\n",
      "3602/3602 [==============================] - 1s 345us/step - loss: -3.2696 - acc: 0.7479\n",
      "Epoch 10/150\n",
      "3602/3602 [==============================] - 1s 312us/step - loss: -3.2666 - acc: 0.7479\n",
      "Epoch 11/150\n",
      "3602/3602 [==============================] - 1s 324us/step - loss: -3.2470 - acc: 0.7501\n",
      "Epoch 12/150\n",
      "3602/3602 [==============================] - 1s 316us/step - loss: -3.2838 - acc: 0.75100s - loss: -3.\n",
      "Epoch 13/150\n",
      "3602/3602 [==============================] - 1s 324us/step - loss: -3.3095 - acc: 0.7537\n",
      "Epoch 14/150\n",
      "3602/3602 [==============================] - 1s 321us/step - loss: -3.3218 - acc: 0.7543\n",
      "Epoch 15/150\n",
      "3602/3602 [==============================] - 1s 368us/step - loss: -3.3063 - acc: 0.7546\n",
      "Epoch 16/150\n",
      "3602/3602 [==============================] - 1s 310us/step - loss: -3.3094 - acc: 0.7532\n",
      "Epoch 17/150\n",
      "3602/3602 [==============================] - 1s 389us/step - loss: -3.2961 - acc: 0.7529\n",
      "Epoch 18/150\n",
      "3602/3602 [==============================] - 1s 375us/step - loss: -3.2848 - acc: 0.7510\n",
      "Epoch 19/150\n",
      "3602/3602 [==============================] - 1s 384us/step - loss: -3.2709 - acc: 0.7496\n",
      "Epoch 20/150\n",
      "3602/3602 [==============================] - 1s 361us/step - loss: -3.3032 - acc: 0.7535\n",
      "Epoch 21/150\n",
      "3602/3602 [==============================] - 1s 302us/step - loss: -3.3143 - acc: 0.7560\n",
      "Epoch 22/150\n",
      "3602/3602 [==============================] - 1s 303us/step - loss: -3.3145 - acc: 0.7560\n",
      "Epoch 23/150\n",
      "3602/3602 [==============================] - 1s 303us/step - loss: -3.3188 - acc: 0.7562\n",
      "Epoch 24/150\n",
      "3602/3602 [==============================] - 1s 351us/step - loss: -3.3204 - acc: 0.7574\n",
      "Epoch 25/150\n",
      "3602/3602 [==============================] - 1s 355us/step - loss: -3.3088 - acc: 0.7560\n",
      "Epoch 26/150\n",
      "3602/3602 [==============================] - 1s 400us/step - loss: -3.3014 - acc: 0.7560\n",
      "Epoch 27/150\n",
      "3602/3602 [==============================] - 2s 456us/step - loss: -3.3118 - acc: 0.7551\n",
      "Epoch 28/150\n",
      "3602/3602 [==============================] - 1s 335us/step - loss: -3.2927 - acc: 0.7546\n",
      "Epoch 29/150\n",
      "3602/3602 [==============================] - 1s 349us/step - loss: -3.2735 - acc: 0.7526\n",
      "Epoch 30/150\n",
      "3602/3602 [==============================] - 1s 316us/step - loss: -3.3094 - acc: 0.7543\n",
      "Epoch 31/150\n",
      "3602/3602 [==============================] - 1s 412us/step - loss: -3.3011 - acc: 0.7532\n",
      "Epoch 32/150\n",
      "3602/3602 [==============================] - 1s 347us/step - loss: -3.3296 - acc: 0.7579\n",
      "Epoch 33/150\n",
      "3602/3602 [==============================] - 1s 315us/step - loss: -3.3276 - acc: 0.7582\n",
      "Epoch 34/150\n",
      "3602/3602 [==============================] - 1s 317us/step - loss: -3.3333 - acc: 0.7576\n",
      "Epoch 35/150\n",
      "3602/3602 [==============================] - 1s 325us/step - loss: -3.3135 - acc: 0.7565\n",
      "Epoch 36/150\n",
      "3602/3602 [==============================] - 1s 346us/step - loss: -3.3121 - acc: 0.7565\n",
      "Epoch 37/150\n",
      "3602/3602 [==============================] - 2s 423us/step - loss: -3.3183 - acc: 0.7565\n",
      "Epoch 38/150\n",
      "3602/3602 [==============================] - 1s 329us/step - loss: -3.3235 - acc: 0.7574\n",
      "Epoch 39/150\n",
      "3602/3602 [==============================] - 1s 331us/step - loss: -3.3227 - acc: 0.7576\n",
      "Epoch 40/150\n",
      "3602/3602 [==============================] - 1s 343us/step - loss: -3.3214 - acc: 0.7576\n",
      "Epoch 41/150\n",
      "3602/3602 [==============================] - 1s 326us/step - loss: -3.3077 - acc: 0.7562\n",
      "Epoch 42/150\n",
      "3602/3602 [==============================] - 1s 329us/step - loss: -3.3096 - acc: 0.7543\n",
      "Epoch 43/150\n",
      "3602/3602 [==============================] - 1s 310us/step - loss: -3.3134 - acc: 0.7551\n",
      "Epoch 44/150\n",
      "3602/3602 [==============================] - 1s 333us/step - loss: -3.3205 - acc: 0.7571\n",
      "Epoch 45/150\n",
      "3602/3602 [==============================] - 1s 388us/step - loss: -3.3301 - acc: 0.7579\n",
      "Epoch 46/150\n",
      "3602/3602 [==============================] - 1s 312us/step - loss: -3.3188 - acc: 0.7579\n",
      "Epoch 47/150\n",
      "3602/3602 [==============================] - 1s 326us/step - loss: -3.3155 - acc: 0.7557\n",
      "Epoch 48/150\n",
      "3602/3602 [==============================] - 1s 322us/step - loss: -3.3027 - acc: 0.7574\n",
      "Epoch 49/150\n",
      "3602/3602 [==============================] - 1s 325us/step - loss: -3.3187 - acc: 0.7574\n",
      "Epoch 50/150\n",
      "3602/3602 [==============================] - 1s 334us/step - loss: -3.3084 - acc: 0.7560\n",
      "Epoch 51/150\n",
      "3602/3602 [==============================] - 1s 326us/step - loss: -3.3188 - acc: 0.7574\n",
      "Epoch 52/150\n",
      "3602/3602 [==============================] - 1s 329us/step - loss: -3.3189 - acc: 0.7574\n",
      "Epoch 53/150\n",
      "3602/3602 [==============================] - 2s 445us/step - loss: -3.3194 - acc: 0.7576\n",
      "Epoch 54/150\n",
      "3602/3602 [==============================] - 2s 497us/step - loss: -3.3200 - acc: 0.7576\n",
      "Epoch 55/150\n",
      "3602/3602 [==============================] - 1s 360us/step - loss: -3.3205 - acc: 0.7576\n",
      "Epoch 56/150\n",
      "3602/3602 [==============================] - 1s 350us/step - loss: -3.3204 - acc: 0.7574\n",
      "Epoch 57/150\n",
      "3602/3602 [==============================] - 1s 363us/step - loss: -3.3200 - acc: 0.7576\n",
      "Epoch 58/150\n",
      "3602/3602 [==============================] - 1s 359us/step - loss: -3.2962 - acc: 0.7551\n",
      "Epoch 59/150\n",
      "3602/3602 [==============================] - 1s 358us/step - loss: -3.3141 - acc: 0.7574\n",
      "Epoch 60/150\n",
      "3602/3602 [==============================] - 1s 370us/step - loss: -3.3128 - acc: 0.7568\n",
      "Epoch 61/150\n",
      "3602/3602 [==============================] - 1s 329us/step - loss: -3.3145 - acc: 0.7568\n",
      "Epoch 62/150\n",
      "3602/3602 [==============================] - 1s 334us/step - loss: -3.3154 - acc: 0.7574\n",
      "Epoch 63/150\n",
      "3602/3602 [==============================] - 1s 328us/step - loss: -3.3150 - acc: 0.7574\n",
      "Epoch 64/150\n",
      "3602/3602 [==============================] - 1s 353us/step - loss: -3.3156 - acc: 0.7574\n",
      "Epoch 65/150\n",
      "3602/3602 [==============================] - 1s 334us/step - loss: -3.3144 - acc: 0.75650s -\n",
      "Epoch 66/150\n",
      "3602/3602 [==============================] - 1s 343us/step - loss: -3.3115 - acc: 0.7568\n",
      "Epoch 67/150\n",
      "3602/3602 [==============================] - 1s 409us/step - loss: -3.3144 - acc: 0.7568\n",
      "Epoch 68/150\n",
      "3602/3602 [==============================] - 1s 334us/step - loss: -3.3149 - acc: 0.7574\n",
      "Epoch 69/150\n",
      "3602/3602 [==============================] - 1s 332us/step - loss: -3.2902 - acc: 0.7546\n",
      "Epoch 70/150\n",
      "3602/3602 [==============================] - 1s 347us/step - loss: -3.3061 - acc: 0.7549\n",
      "Epoch 71/150\n",
      "3602/3602 [==============================] - 1s 336us/step - loss: -3.2777 - acc: 0.7549\n",
      "Epoch 72/150\n",
      "3602/3602 [==============================] - 1s 333us/step - loss: -3.2641 - acc: 0.7549\n",
      "Epoch 73/150\n",
      "3602/3602 [==============================] - 1s 335us/step - loss: -3.2893 - acc: 0.7549\n",
      "Epoch 74/150\n",
      "3602/3602 [==============================] - 1s 389us/step - loss: -3.2967 - acc: 0.7560\n",
      "Epoch 75/150\n",
      "3602/3602 [==============================] - 1s 325us/step - loss: -3.3005 - acc: 0.7565\n",
      "Epoch 76/150\n",
      "3602/3602 [==============================] - 1s 341us/step - loss: -3.3021 - acc: 0.7560\n",
      "Epoch 77/150\n",
      "3602/3602 [==============================] - 1s 330us/step - loss: -3.3028 - acc: 0.7562\n",
      "Epoch 78/150\n",
      "3602/3602 [==============================] - 1s 324us/step - loss: -3.3023 - acc: 0.7568\n",
      "Epoch 79/150\n",
      "3602/3602 [==============================] - 1s 323us/step - loss: -3.3032 - acc: 0.7568\n",
      "Epoch 80/150\n",
      "3602/3602 [==============================] - 1s 327us/step - loss: -3.3030 - acc: 0.7546\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3602/3602 [==============================] - 1s 317us/step - loss: -3.3033 - acc: 0.7565\n",
      "Epoch 82/150\n",
      "3602/3602 [==============================] - 1s 305us/step - loss: -3.3010 - acc: 0.7568\n",
      "Epoch 83/150\n",
      "3602/3602 [==============================] - 1s 308us/step - loss: -3.2927 - acc: 0.7554\n",
      "Epoch 84/150\n",
      "3602/3602 [==============================] - 1s 300us/step - loss: -3.2929 - acc: 0.7560\n",
      "Epoch 85/150\n",
      "3602/3602 [==============================] - 1s 306us/step - loss: -3.2934 - acc: 0.7560\n",
      "Epoch 86/150\n",
      "3602/3602 [==============================] - 1s 308us/step - loss: -3.2815 - acc: 0.7557\n",
      "Epoch 87/150\n",
      "3602/3602 [==============================] - 1s 309us/step - loss: -3.2840 - acc: 0.7554\n",
      "Epoch 88/150\n",
      "3602/3602 [==============================] - 1s 306us/step - loss: -3.2849 - acc: 0.7549\n",
      "Epoch 89/150\n",
      "3602/3602 [==============================] - 1s 307us/step - loss: -3.2926 - acc: 0.7557\n",
      "Epoch 90/150\n",
      "3602/3602 [==============================] - 1s 312us/step - loss: -3.2923 - acc: 0.7560\n",
      "Epoch 91/150\n",
      "3602/3602 [==============================] - 1s 311us/step - loss: -3.2985 - acc: 0.7557\n",
      "Epoch 92/150\n",
      "3602/3602 [==============================] - 1s 327us/step - loss: -3.2978 - acc: 0.7565\n",
      "Epoch 93/150\n",
      "3602/3602 [==============================] - 1s 315us/step - loss: -3.2932 - acc: 0.7562\n",
      "Epoch 94/150\n",
      "3602/3602 [==============================] - 1s 311us/step - loss: -3.2978 - acc: 0.7562\n",
      "Epoch 95/150\n",
      "3602/3602 [==============================] - 1s 308us/step - loss: -3.2980 - acc: 0.7565\n",
      "Epoch 96/150\n",
      "3602/3602 [==============================] - 1s 313us/step - loss: -3.2980 - acc: 0.7565\n",
      "Epoch 97/150\n",
      "3602/3602 [==============================] - 1s 308us/step - loss: -3.2982 - acc: 0.7565\n",
      "Epoch 98/150\n",
      "3602/3602 [==============================] - 1s 308us/step - loss: -3.2980 - acc: 0.7562\n",
      "Epoch 99/150\n",
      "3602/3602 [==============================] - 1s 317us/step - loss: -3.2977 - acc: 0.7560\n",
      "Epoch 100/150\n",
      "3602/3602 [==============================] - 1s 311us/step - loss: -3.2932 - acc: 0.7557\n",
      "Epoch 101/150\n",
      "3602/3602 [==============================] - 1s 311us/step - loss: -3.2941 - acc: 0.7549\n",
      "Epoch 102/150\n",
      "3602/3602 [==============================] - 1s 315us/step - loss: -3.2895 - acc: 0.7557\n",
      "Epoch 103/150\n",
      "3602/3602 [==============================] - 1s 312us/step - loss: -3.2984 - acc: 0.7554\n",
      "Epoch 104/150\n",
      "3602/3602 [==============================] - 1s 311us/step - loss: -3.3060 - acc: 0.7562\n",
      "Epoch 105/150\n",
      "3602/3602 [==============================] - 1s 317us/step - loss: -3.3064 - acc: 0.7568\n",
      "Epoch 106/150\n",
      "3602/3602 [==============================] - 1s 317us/step - loss: -3.3028 - acc: 0.7560\n",
      "Epoch 107/150\n",
      "3602/3602 [==============================] - 1s 321us/step - loss: -3.3058 - acc: 0.7565\n",
      "Epoch 108/150\n",
      "3602/3602 [==============================] - 1s 311us/step - loss: -3.3068 - acc: 0.7562\n",
      "Epoch 109/150\n",
      "3602/3602 [==============================] - 1s 310us/step - loss: -3.3069 - acc: 0.7571\n",
      "Epoch 110/150\n",
      "3602/3602 [==============================] - 1s 313us/step - loss: -3.3067 - acc: 0.7568\n",
      "Epoch 111/150\n",
      "3602/3602 [==============================] - 1s 313us/step - loss: -3.3064 - acc: 0.7562\n",
      "Epoch 112/150\n",
      "3602/3602 [==============================] - 1s 316us/step - loss: -3.3075 - acc: 0.7562\n",
      "Epoch 113/150\n",
      "3602/3602 [==============================] - 1s 311us/step - loss: -3.3037 - acc: 0.7565\n",
      "Epoch 114/150\n",
      "3602/3602 [==============================] - 1s 316us/step - loss: -3.3069 - acc: 0.7565\n",
      "Epoch 115/150\n",
      "3602/3602 [==============================] - 1s 312us/step - loss: -3.3070 - acc: 0.7562\n",
      "Epoch 116/150\n",
      "3602/3602 [==============================] - 1s 312us/step - loss: -3.3074 - acc: 0.7571\n",
      "Epoch 117/150\n",
      "3602/3602 [==============================] - 1s 312us/step - loss: -3.3070 - acc: 0.7571\n",
      "Epoch 118/150\n",
      "3602/3602 [==============================] - 1s 313us/step - loss: -3.3070 - acc: 0.7565\n",
      "Epoch 119/150\n",
      "3602/3602 [==============================] - 1s 317us/step - loss: -3.3072 - acc: 0.7565\n",
      "Epoch 120/150\n",
      "3602/3602 [==============================] - 1s 317us/step - loss: -3.3068 - acc: 0.7568\n",
      "Epoch 121/150\n",
      "3602/3602 [==============================] - 1s 324us/step - loss: -3.3053 - acc: 0.7565\n",
      "Epoch 122/150\n",
      "3602/3602 [==============================] - 1s 316us/step - loss: -3.3026 - acc: 0.7565\n",
      "Epoch 123/150\n",
      "3602/3602 [==============================] - 1s 319us/step - loss: -3.2873 - acc: 0.7543\n",
      "Epoch 124/150\n",
      "3602/3602 [==============================] - 1s 317us/step - loss: -3.2855 - acc: 0.7549\n",
      "Epoch 125/150\n",
      "3602/3602 [==============================] - 1s 311us/step - loss: -3.3146 - acc: 0.7574\n",
      "Epoch 126/150\n",
      "3602/3602 [==============================] - 1s 320us/step - loss: -3.3059 - acc: 0.7560\n",
      "Epoch 127/150\n",
      "3602/3602 [==============================] - 1s 329us/step - loss: -3.3192 - acc: 0.7571\n",
      "Epoch 128/150\n",
      "3602/3602 [==============================] - 1s 334us/step - loss: -3.3199 - acc: 0.7568\n",
      "Epoch 129/150\n",
      "3602/3602 [==============================] - 1s 320us/step - loss: -3.3192 - acc: 0.7574\n",
      "Epoch 130/150\n",
      "3602/3602 [==============================] - 1s 319us/step - loss: -3.3208 - acc: 0.7582\n",
      "Epoch 131/150\n",
      "3602/3602 [==============================] - 1s 321us/step - loss: -3.3234 - acc: 0.7579\n",
      "Epoch 132/150\n",
      "3602/3602 [==============================] - 1s 316us/step - loss: -3.3023 - acc: 0.7565\n",
      "Epoch 133/150\n",
      "3602/3602 [==============================] - 1s 316us/step - loss: -3.3330 - acc: 0.7579\n",
      "Epoch 134/150\n",
      "3602/3602 [==============================] - 1s 322us/step - loss: -3.3340 - acc: 0.7587\n",
      "Epoch 135/150\n",
      "3602/3602 [==============================] - 1s 323us/step - loss: -3.3557 - acc: 0.7599\n",
      "Epoch 136/150\n",
      "3602/3602 [==============================] - 1s 318us/step - loss: -3.3546 - acc: 0.7593\n",
      "Epoch 137/150\n",
      "3602/3602 [==============================] - 1s 325us/step - loss: -3.3552 - acc: 0.7593\n",
      "Epoch 138/150\n",
      "3602/3602 [==============================] - 1s 347us/step - loss: -3.3566 - acc: 0.7599\n",
      "Epoch 139/150\n",
      "3602/3602 [==============================] - 1s 343us/step - loss: -3.3567 - acc: 0.7596\n",
      "Epoch 140/150\n",
      "3602/3602 [==============================] - 1s 357us/step - loss: -3.3566 - acc: 0.7599\n",
      "Epoch 141/150\n",
      "3602/3602 [==============================] - 1s 361us/step - loss: -3.3565 - acc: 0.7601\n",
      "Epoch 142/150\n",
      "3602/3602 [==============================] - 1s 341us/step - loss: -3.3565 - acc: 0.7601\n",
      "Epoch 143/150\n",
      "3602/3602 [==============================] - 1s 325us/step - loss: -3.3566 - acc: 0.7599\n",
      "Epoch 144/150\n",
      "3602/3602 [==============================] - 1s 318us/step - loss: -3.3565 - acc: 0.7599\n",
      "Epoch 145/150\n",
      "3602/3602 [==============================] - 1s 318us/step - loss: -3.3568 - acc: 0.7599\n",
      "Epoch 146/150\n",
      "3602/3602 [==============================] - 1s 323us/step - loss: -3.3558 - acc: 0.7601\n",
      "Epoch 147/150\n",
      "3602/3602 [==============================] - 1s 321us/step - loss: -3.3564 - acc: 0.7596\n",
      "Epoch 148/150\n",
      "3602/3602 [==============================] - 1s 324us/step - loss: -3.3564 - acc: 0.7599\n",
      "Epoch 149/150\n",
      "3602/3602 [==============================] - 1s 334us/step - loss: -3.3565 - acc: 0.7601\n",
      "Epoch 150/150\n",
      "3602/3602 [==============================] - 1s 337us/step - loss: -3.3537 - acc: 0.7587\n",
      "3602/3602 [==============================] - 0s 95us/step\n",
      "\n",
      "acc: 75.96%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "# neurons = 4\n",
    "# batch_size = 1\n",
    "# layer = LSTM(units = 4, stateful=True)\n",
    "# print(layer)\n",
    "\n",
    "numpy.random.seed(7)\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=3612, activation='relu'))\n",
    "model.add(Dense(3612, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "'''The ***batch size*** limits the number of samples to be shown to the network before a weight update can be performed. \n",
    "This same limitation is then imposed when making predictions with the fit model.\n",
    "Specifically, the batch size used when fitting your model controls how many predictions you must make at a time.'''\n",
    "\n",
    "model.fit(X, Y, epochs=150, batch_size=1)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.4f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
