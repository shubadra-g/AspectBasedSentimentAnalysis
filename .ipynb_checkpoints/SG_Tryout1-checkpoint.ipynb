{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concern\n",
      "(3602, 3612)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Apr  6 16:09:25 2018\n",
    "\n",
    "@author: Shubadra\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter \n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "# def get_pos( word ):\n",
    "#     w_synsets = wordnet.synsets(word)\n",
    "\n",
    "#     pos_counts = Counter()\n",
    "#     pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "#     pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "#     pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "#     pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "#     most_common_pos_list = pos_counts.most_common(3)\n",
    "#     return most_common_pos_list[0][0]\n",
    "\n",
    "find = lambda searchList, elem: [[i for i, x in enumerate(searchList) if x == e] for e in elem]\n",
    "fname = 'project_2_train/' + 'data 2_train.csv'\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_set = {'i', 'shan', 'just', 'how', 'each', 'out', 'themselves', 'their', 'before', 'were', 'very', 'as', 'further', 'his', 'a', 'once', 'youve', 'y', 'is', 'shouldve', 'youll', 'on', 'd', 'm', 'under', 'haven', 'which', 'only', 'them', 'was', 'by', 'needn', 'whom', 'that', 'when', 's', 'isn', 'its', 'no', 'wasn', 'in', 'we', 'theirs', 'those', 'this', 'having', 'and', 'ain', 'most', 'up', 'off', 'being', 'aren', 'shouldn', 'ourselves', 'from', 'down', 'herself', 'her', 'you', 'are', 'its', 'who', 'the', 'here', 'where', 'your', 'youd', 'she', 'didn', 'weren', 'about', 'has', 'our', 'an', 'yourselves', 'or', 'hasn', 'again', 'while', 'does', 'him', 'shes', 'above', 'below', 'itself', 'to', 'through', 'will', 'couldn', 'hers', 'they', 'doing', 'because', 'he', 'what', 'such', 'youre', 'nor', 'too', 'should', 'ours', 'then', 'himself', 'all', 'of', 'mightn', 'between', 'now', 'against', 'some', 'with', 'until', 'am', 'other', 'at', 'can', 'over', 'mustn', 'wouldn', 'do', 'for', 'after', 'hadn', 'me', 'been', 'same', 'doesn', 'my', 'these', 'll', 'did', 'had', 'it', 'so', 'ma', 'during', 'than', 'o', 'yourself', 'own', 'have', 're', 've', 'be', 'why', 't', 'there', 'more', 'won', 'yours', 'few', 'into', 'thatll', 'any', 'myself', 'both', 'don', 'if'}\n",
    "# print(stopwords_set)\n",
    "# stopwords_set.remove('but')\n",
    "# stopwords_set.remove('not')\n",
    "\n",
    "sentence_weights = []\n",
    "X, Y, X1 = [], [], []\n",
    "\n",
    "f = open(fname, 'r')\n",
    "for i, line in enumerate(f):\n",
    "    if i != 0:\n",
    "        ''' Splitting the columns based on comma - since it is csv'''\n",
    "        columns = line.split(',')\n",
    "\n",
    "        ''' The comma in the actual sentence was represented as [commma] because of csv format, replace that '''\n",
    "        columns[1] = columns[1].replace('[comma]', ',')\n",
    "        columns[2] = columns[2].replace('[comma]', ',')\n",
    "\n",
    "        ''' Some aspect terms are mic in the sentence where computer/mic is present '''\n",
    "        columns[1] = columns[1].replace('-', ' ')\n",
    "        columns[2] = columns[2].replace('-', ' ')\n",
    "        columns[1] = columns[1].replace('/', ' ')\n",
    "        columns[2] = columns[2].replace('/', ' ')\n",
    "\n",
    "        ''' NOt used anymore'''\n",
    "        tokenizer = RegexpTokenizer(r'\\w+') # doesn't work if the special char is in the token\n",
    "        # columns[2] = tokenizer.tokenize(columns[2].lower())\n",
    "\n",
    "        '''Tokenize the words'''\n",
    "        columns[1] = word_tokenize(columns[1])\n",
    "        columns[2] = word_tokenize(columns[2])\n",
    "\n",
    "        for j, elem in enumerate(columns[1]):\n",
    "            '''Remove special characters'''\n",
    "            columns[1][j] = re.sub('[^0-9a-zA-Z]+', '', elem).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[1][j] == 'nt':\n",
    "                columns[1][j] = 'not'\n",
    "\n",
    "        for j, elem in enumerate(columns[2]):\n",
    "            '''Remove special characters'''\n",
    "            columns[2][j] = re.sub('[^0-9a-zA-Z]+', '', elem).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[2][j] == 'nt':\n",
    "                columns[2][j] = 'not'\n",
    "\n",
    "        '''Remove empty string tokens'''\n",
    "        columns[1] = [x.strip() for x in columns[1] if x.strip() != '']\n",
    "        columns[2] = [x.strip() for x in columns[2] if x.strip() != '']\n",
    "\n",
    "        ''' Remove stop words '''\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        columns[2] = [word for word in columns[2] if word not in stopwords_set]\n",
    "        \n",
    "#         '''Lemmatizing followed by Stemming'''\n",
    "#         ps = PorterStemmer()\n",
    "#         wnl = WordNetLemmatizer()\n",
    "#         for word_index, word in enumerate(columns[1]):\n",
    "#             columns[1][word_index] = ps.stem(word)\n",
    "#         for aspect_term_index, aspect_term in enumerate(columns[2]):\n",
    "#             columns[1][aspect_term_index] = ps.stem(wnl.lemmatize(aspect_term))\n",
    " \n",
    "        columns[1] = ' '.join(columns[1])\n",
    "        columns[2] = ' '.join(columns[2])\n",
    "\n",
    "        ''' The aspect term location given is not proper - hence extracting the location by ourselves '''\n",
    "        columns[3] = []\n",
    "        ''' Finds the location of the aspect term in the sentence '''\n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "            ''' since some aspect terms are not surrounded by spaces, hence below code\n",
    "            NOT USED ANYMORE\n",
    "            '''\n",
    "            if m.start() != 0 and columns[1][m.start() - 1] != ' ':\n",
    "                # columns[1] = columns[1][:m.start()] + ' ' + columns[1][m.start():]\n",
    "                pass\n",
    "\n",
    "        ''' Because of adding space in above code, the position is messed up, hence redoing it. '''\n",
    "        columns[3] = []\n",
    "\n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "\n",
    "        ''' Tokenizing the words again '''\n",
    "        columns[1] = columns[1].split(' ')\n",
    "        columns[2] = columns[2].split(' ')\n",
    "        ''' New column to specify the aspect term location in the list '''\n",
    "        columns.append([])\n",
    "        # columns[5] = []\n",
    "        ''' for multiple positions of aspect term '''\n",
    "        for elem in columns[3]:\n",
    "            if elem[0] == 0: #start index of the 1st occurance of the aspect term\n",
    "                columns[5].append([0])\n",
    "            else:\n",
    "                temp_len = elem[0] # assign the start position of the aspect term\n",
    "                for j, tokens in enumerate(columns[1]):\n",
    "                    temp_len -= (len(tokens) + 1) # Counting the words till the position - +1 for blank space\n",
    "                    if temp_len == 0: # Reached the aspect word\n",
    "                        columns[5].append([j+1])\n",
    "                        break\n",
    "            for k in range(len(columns[2])-1): # if multiple words in the aspect term - tag the following words in the sentence\n",
    "                columns[5][-1].append(columns[5][-1][-1]+1)\n",
    "\n",
    "        ''' if not found the aspect term location in the list then do '''\n",
    "        if len(columns[5]) < 1:\n",
    "            continue\n",
    "\n",
    "            # pass\n",
    "\n",
    "        word_wt_list = [[elem, 0] for elem in columns[1]]\n",
    "        ''' Assigning weights to every word based on the distance from the aspect term '''\n",
    "        for j, elem in enumerate(columns[5]):\n",
    "            for k, word in enumerate(columns[1]):\n",
    "                if k < elem[0]: # Word left to the aspect term\n",
    "                    dist = abs(elem[0] - k)\n",
    "                    if word_wt_list[k][1] < 1/dist:\n",
    "                        word_wt_list[k][1] = 1/dist\n",
    "                elif k > elem[-1]: # word right to the aspect term : Aspect term can have multiple words\n",
    "                    dist = abs(k - elem[-1])\n",
    "                    if word_wt_list[k][1] < 1/dist:\n",
    "                        word_wt_list[k][1] = 1/dist\n",
    "            for aspect_word_loc in elem:\n",
    "                word_wt_list[aspect_word_loc][1] = 1.5\n",
    "\n",
    "        ''' For duplicate words - if it's aspect term - add the weights, if not, then take the weight which is greater, make the other weight as zero '''\n",
    "        for j, word_1 in enumerate(word_wt_list):\n",
    "            for k, word_2 in enumerate(word_wt_list[j+1:]):\n",
    "                if word_1[0] == word_2[0]:\n",
    "                    if word_1[1] == 1.5 and word_2[1] == 1.5:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_1[1] += 1.5\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] < word_2[1]:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_1[1] = word_2[1]\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] > word_2[1]:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] == word_2[1]:\n",
    "                        word_2[1] = 0\n",
    "\n",
    "\n",
    "        ''' For removing duplicate words - they have weight zero now'''\n",
    "        word_wt_list = [elem for elem in word_wt_list if elem[1] != 0]\n",
    "\n",
    "        if len(columns[2]) > 1 and len(columns[5]) > 1:\n",
    "            # print(line)\n",
    "            # print(columns)\n",
    "            # print(word_wt_list)\n",
    "            pass\n",
    "\n",
    "        if columns[0] == '2911_0':\n",
    "            # print(line)\n",
    "            # print(word_wt_list)\n",
    "            pass\n",
    "        if i == 100:\n",
    "            # break\n",
    "            pass\n",
    "\n",
    "        '''Removing new line character from columns[4] -  the polarity column'''\n",
    "        columns[4] = columns[4].rstrip('\\n')\n",
    "        X.append(columns[1])\n",
    "        X1.append(' '.join(columns[1]))\n",
    "\n",
    "        Y.append(columns[4])\n",
    "        sentence_weights.append(word_wt_list)\n",
    "\n",
    "\n",
    "'''Building vocabulary and counting word occurances'''\n",
    "X_reduced = reduce(lambda x1, x2: x1 + x2, X)\n",
    "vocab = list(set(X_reduced))\n",
    "print(vocab[0])\n",
    "weight_v = np.zeros_like(vocab, dtype = np.float_)\n",
    "weight_x = []\n",
    "pf = open('weightxy_data1.csv', 'w')\n",
    "\n",
    "pf.write(','.join(vocab))\n",
    "pf.write(',class_')\n",
    "pf.write('\\n')\n",
    "for i, sentence in enumerate(sentence_weights):\n",
    "    for word in sentence:\n",
    "        v_index = vocab.index(word[0])\n",
    "        weight_v[v_index] = word[1]\n",
    "\n",
    "    weight_x.append(weight_v)\n",
    "    if len(sentence) != weight_v[np.where(weight_v > 0)].shape[0]:\n",
    "        print(sentence)\n",
    "        print(len(sentence))\n",
    "        print(weight_v[np.where(weight_v > 0)])\n",
    "\n",
    "    ''' code to write the data to file '''\n",
    "    temp = [str(j) for j in weight_v.tolist()]\n",
    "    pf.write(','.join(temp))\n",
    "    pf.write(','+str(Y[i]))\n",
    "    pf.write('\\n')\n",
    "    weight_v = np.zeros_like(weight_v)\n",
    "\n",
    "pf.close()\n",
    "weight_x = np.array(weight_x)\n",
    "\n",
    "print(weight_x.shape)\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "# weight_X2 = np.reshape(weight_X, (len(Y) ,weight_X.shape[0]))\n",
    "# print(weight_X2.shape)\n",
    "# tokens = f.split(',') # columns\n",
    "# remove punctuations\n",
    "# table = str.maketrans('', '', string.punctuation)\n",
    "# print(table)\n",
    "# tokens = [w.translate(table) for w in tokens]\n",
    "# # remove tokens that are not alphbetic\n",
    "# tokens = [word for word in tokens if word.isalpha()]\n",
    "#\n",
    "# # remove stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tokens = [w for w in tokens if not w in stop_words]\n",
    "# # filter out short tokens\n",
    "# tokens = [word for word in tokens if len(word) > 1]\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3602, 3613)\n",
      "(3602, 3612)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load data set'''\n",
    "df1 = pd.read_csv(\"weightxy_data1.csv\")\n",
    "print(df1.shape)\n",
    "print(df1.iloc[:,:-1].shape)\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "(3602, 3612)\n",
      "(3602,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(df1.iloc[:,:-1], df1['class_'], test_size = 0.3, random_state =42)\n",
    "X, Y = df1.iloc[:,:-1], df1['class_']\n",
    "\n",
    "# svm_clf = svm.SVC(kernel = 'rbf')\n",
    "svm_clf = svm.LinearSVC(penalty = 'l2', C = 0.5, random_state = 0)\n",
    "\n",
    "# svm_clf.fit(X, Y)\n",
    "print('success')\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([ 0.28974366,  0.26565385,  0.28128076,  0.27639508,  0.27071881,\n",
      "        0.26224685,  0.2632997 ,  0.27270174,  0.27185178,  0.26565242]), 'score_time': array([ 0.005023  ,  0.        ,  0.        ,  0.00401115,  0.        ,\n",
      "        0.01562738,  0.        ,  0.        ,  0.0040102 ,  0.        ]), 'test_score': array([ 0.72099448,  0.71270718,  0.70718232,  0.63711911,  0.70833333,\n",
      "        0.68523677,  0.68523677,  0.64902507,  0.68523677,  0.68245125]), 'train_score': array([ 0.96265432,  0.96728395,  0.96018519,  0.96667695,  0.96360271,\n",
      "        0.96299722,  0.96577243,  0.96731422,  0.96515572,  0.96453901])}\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cross_validate(svm_clf, X, Y, cv = 10, scoring = 'accuracy')\n",
    "\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...,  1  0  0]\n",
      "success\n",
      "\n",
      "\n",
      "Model Accuracy is: 68.7396%\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.56      0.54       732\n",
      "          1       0.36      0.44      0.40       517\n",
      "         -1       0.85      0.78      0.81      2353\n",
      "\n",
      "avg / total       0.71      0.69      0.70      3602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Cross validation'''\n",
    "y_pred = cross_val_predict(svm_clf, X, Y, cv = 10) #, scoring = 'accuracy')\n",
    "\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "\n",
    "mod_accracy = accuracy_score(y_pred, Y)\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(y_pred, Y, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n",
      "success\n",
      "{'fit_time': array([ 5.12622952,  5.11323261,  6.04007006,  6.02486467,  5.91393495,\n",
      "        6.36458635,  6.58375454,  6.0492177 ,  6.22554684,  5.44387674]), 'score_time': array([ 0.03130102,  0.04688334,  0.03574657,  0.08221912,  0.0312891 ,\n",
      "        0.04683113,  0.03125858,  0.1383698 ,  0.06822801,  0.0505023 ]), 'test_score': array([ 0.67127072,  0.63535912,  0.64917127,  0.63157895,  0.68333333,\n",
      "        0.64066852,  0.66573816,  0.64623955,  0.63231198,  0.66852368]), 'train_score': array([ 0.77438272,  0.77561728,  0.76851852,  0.77321814,  0.76588526,\n",
      "        0.77489978,  0.77489978,  0.7613321 ,  0.76657416,  0.76688252])}\n",
      "[-1 -1  1 ..., -1 -1 -1]\n",
      "success\n",
      "\n",
      "\n",
      "Model Accuracy is: 64.6585%\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.47      0.42       634\n",
      "          1       0.27      0.45      0.34       386\n",
      "         -1       0.86      0.72      0.78      2582\n",
      "\n",
      "avg / total       0.71      0.65      0.67      3602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "adaBoost_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2)) #, n_estimators=2, learning_rate=1)\n",
    "scores = cross_validate(adaBoost_clf, X, Y, cv = 10)\n",
    "print(scores.keys())\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "print(scores)\n",
    "\n",
    "y_pred = cross_val_predict(adaBoost_clf, X, Y, cv = 10) #, scoring = 'accuracy')\n",
    "\n",
    "print(y_pred)\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "\n",
    "mod_accracy = accuracy_score(y_pred, Y)\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(y_pred, Y, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''calculate the precision recall and stuff'''\n",
    "\n",
    "# t_precision = precision_score(Y_test, t_pred, average='weighted')\n",
    "# t_recall = recall_score(Y_test, t_pred, average='weighted')\n",
    "# t_f1_score = f1_score(Y_test, t_pred, average='weighted')\n",
    "\n",
    "# print('*** Scores for SVM: ***')\n",
    "# target_names = ['0', '1', '-1']\n",
    "# print('Pr, Re scores wrt each class')\n",
    "# print(classification_report(Y_test, t_pred, target_names=target_names))\n",
    "\n",
    "# print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "# print('\\n\\nPrecision Score is: {:.4%}' .format(t_precision))\n",
    "# print('\\n\\nRecall Score is: {:.4%}' .format(t_recall))\n",
    "# print('\\n\\nF1-Score is: {:.4%}' .format(t_f1_score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
