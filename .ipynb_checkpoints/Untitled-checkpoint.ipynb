{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 361  362  363 ..., 3599 3600 3601] TEST: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Apr  6 16:09:25 2018\n",
    "\n",
    "@author: Shubadra\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "find = lambda searchList, elem: [[i for i, x in enumerate(searchList) if x == e] for e in elem]\n",
    "pfname = 'project_2_train/' + 'parsed_data.txt'\n",
    "fname = 'project_2_train/' + 'data 2_train.csv'\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_set = {'i', 'shan', 'just', 'how', 'each', 'out', 'themselves', 'their', 'before', 'were', 'very', 'as', 'further', 'his', 'a', 'once', 'youve', 'y', 'is', 'shouldve', 'youll', 'on', 'd', 'm', 'under', 'haven', 'which', 'only', 'them', 'was', 'by', 'needn', 'whom', 'that', 'when', 's', 'isn', 'its', 'no', 'wasn', 'in', 'we', 'theirs', 'those', 'this', 'having', 'and', 'ain', 'most', 'up', 'off', 'being', 'aren', 'shouldn', 'ourselves', 'from', 'down', 'herself', 'her', 'you', 'are', 'its', 'who', 'the', 'here', 'where', 'your', 'youd', 'she', 'didn', 'weren', 'about', 'has', 'our', 'an', 'yourselves', 'or', 'hasn', 'again', 'while', 'does', 'him', 'shes', 'above', 'below', 'itself', 'to', 'through', 'will', 'couldn', 'hers', 'they', 'doing', 'because', 'he', 'what', 'such', 'youre', 'nor', 'too', 'should', 'ours', 'then', 'himself', 'all', 'of', 'mightn', 'between', 'now', 'against', 'some', 'with', 'until', 'am', 'other', 'at', 'can', 'over', 'mustn', 'wouldn', 'do', 'for', 'after', 'hadn', 'me', 'been', 'same', 'doesn', 'my', 'these', 'll', 'did', 'had', 'it', 'so', 'ma', 'during', 'than', 'o', 'yourself', 'own', 'have', 're', 've', 'be', 'why', 't', 'there', 'more', 'won', 'yours', 'few', 'into', 'thatll', 'any', 'myself', 'both', 'don', 'if'}\n",
    "# print(stopwords_set)\n",
    "# stopwords_set.remove('but')\n",
    "# stopwords_set.remove('not')\n",
    "pf = open(pfname, 'w')\n",
    "\n",
    "X, Y, X1 = [], [], []\n",
    "# print(X, Y)\n",
    "f = open(fname, 'r')\n",
    "for i, line in enumerate(f):\n",
    "    if i != 0:\n",
    "        # print('Processing data...')\n",
    "        ''' Splitting the columns based on comma - since it is csv'''\n",
    "        columns = line.split(',')\n",
    "\n",
    "        ''' The comma in the actual sentence was represented as [commma] because of csv format, replace that '''\n",
    "        columns[1] = columns[1].replace('[comma]', ',')\n",
    "        columns[2] = columns[2].replace('[comma]', ',')\n",
    "\n",
    "        ''' Some aspect terms are mic in the sentence where computer/mic is present '''\n",
    "        columns[1] = columns[1].replace('-', ' ')\n",
    "        columns[2] = columns[2].replace('-', ' ')\n",
    "        columns[1] = columns[1].replace('/', ' ')\n",
    "        columns[2] = columns[2].replace('/', ' ')\n",
    "\n",
    "        ''' NOt used anymore'''\n",
    "        # tokenizer = RegexpTokenizer(r'\\w+') # doesn't work if the special char is in the token\n",
    "        # columns[2] = tokenizer.tokenize(columns[2].lower())\n",
    "\n",
    "        '''Tokenize the words'''\n",
    "        columns[1] = word_tokenize(columns[1])\n",
    "        columns[2] = word_tokenize(columns[2])\n",
    "\n",
    "        '''For sentence'''\n",
    "        for j, elem in enumerate(columns[1]):\n",
    "            '''Remove special characters'''\n",
    "            columns[1][j] = re.sub('[^0-9a-zA-Z]+', '', elem).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[1][j] == 'nt':\n",
    "                columns[1][j] = 'not'\n",
    "\n",
    "        '''For aspect term'''\n",
    "        for j, elem in enumerate(columns[2]):\n",
    "            '''Remove special characters'''\n",
    "            columns[2][j] = re.sub('[^0-9a-zA-Z]+', '', elem).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[2][j] == 'nt':\n",
    "                columns[2][j] = 'not'\n",
    "\n",
    "        '''Remove empty string tokens'''\n",
    "        columns[1] = [x for x in columns[1] if x != '']\n",
    "        columns[2] = [x for x in columns[2] if x != '']\n",
    "\n",
    "        ''' Remove stop words '''\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        columns[2] = [word for word in columns[2] if word not in stopwords_set]\n",
    "\n",
    "        columns[1] = ' '.join(columns[1])\n",
    "        columns[2] = ' '.join(columns[2])\n",
    "\n",
    "        ''' The aspect term location given is not proper - hence extracting the location by ourselves '''\n",
    "        columns[3] = []\n",
    "        ''' Finds the location of the aspect term in the sentence '''\n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "            ''' since some aspect terms are not surrounded by spaces, hence below code '''\n",
    "            if m.start() != 0 and columns[1][m.start() - 1] != ' ':\n",
    "                columns[1] = columns[1][:m.start()] + ' ' + columns[1][m.start():]\n",
    "\n",
    "        ''' Because of adding space in above code, the position is messed up, hence redoing it. '''\n",
    "        columns[3] = []\n",
    "\n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "\n",
    "        ''' Tokenizing the words again '''\n",
    "        columns[1] = columns[1].split(' ')\n",
    "        columns[2] = columns[2].split(' ')\n",
    "        ''' New column to specify the aspect term location in the list '''\n",
    "        columns.append([])\n",
    "        # columns[5] = []\n",
    "        ''' for multiple positions of the columns '''\n",
    "        for elem in columns[3]:\n",
    "            if elem[0] == 0: #start index of the 1st occurance of the aspect term\n",
    "                columns[5].append([0])\n",
    "            else:\n",
    "                temp_len = elem[0] # assign the start position of the aspect term\n",
    "                for j, tokens in enumerate(columns[1]):\n",
    "                    temp_len -= (len(tokens) + 1) # Counting the words till the position - +1 for blank space\n",
    "                    if temp_len == 0: # Reached the aspect word\n",
    "                        columns[5].append([j+1])\n",
    "                        break\n",
    "            for k in range(len(columns[2])-1): # if multiple words in the aspect term - tag the following words in the sentence\n",
    "                columns[5][-1].append(columns[5][-1][-1]+1)\n",
    "\n",
    "        ''' if not found the aspect term location in the list then do'''\n",
    "        if len(columns[5]) < 1:\n",
    "            print(line)\n",
    "            print(columns)\n",
    "            pass\n",
    "\n",
    "        # if i == 10:\n",
    "        #     # break\n",
    "        #     pass\n",
    "        # # print('***********')\n",
    "        '''Removing new line character from columns[4] -  the polarity column'''\n",
    "        columns[4] = columns[4].rstrip('\\n')\n",
    "        X.append(columns[1])\n",
    "        X1.append(' '.join(columns[1]))\n",
    "\n",
    "        Y.append(columns[4])\n",
    "\n",
    "        # exit()\n",
    "        # print(\"Values in X: \\n\\n\", X)\n",
    "        # print(\"Values in Y: \\n\\n\", Y)\n",
    "\n",
    "'''Building vocabulary and counting word occurances'''\n",
    "X_reduced = reduce(lambda x1, x2: x1 + x2, X)\n",
    "X_ = list(set(X_reduced))\n",
    "# print(len(X_), \"b\")\n",
    "# print(X_)\n",
    "count_vect = CountVectorizer(vocabulary = X_)\n",
    "X_train_counts = count_vect.fit_transform(X1).toarray()\n",
    "for i, xval in enumerate(X_train_counts):\n",
    "    pf.write(','.join(str(xval))) #.join(str(Y[i])))\n",
    "    # print('')\n",
    "    pf.write(' '.join(str(Y[i])))\n",
    "    pf.write('\\n')\n",
    "pf.close()\n",
    "exit()\n",
    "# print(len(X[1122]), len(X_train_counts))\n",
    "# print(\"~~~~~~~~~~~~\")\n",
    "# print(X_train_counts)\n",
    "# print(len(X_train_counts[10]), \"c\")\n",
    "# count = 0\n",
    "# print(\"~~~~~~~~~~~~////////////////////////\")\n",
    "# for val in X_train_counts[1122]:\n",
    "#     if val == 1:\n",
    "#         count += 1\n",
    "# print(count)\n",
    "# print(\"~~~~~~~~~~~~////////////////////////\")\n",
    "# exit()\n",
    "# print(X_train_counts)\n",
    "X_train, X_test, label_train, label_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "'''K-Fold Cross Validation'''\n",
    "kf = KFold(n_splits = 10)\n",
    "# print(kf.get_n_splits(X))\n",
    "# print(kf)\n",
    "X = np.asarray(X_train_counts)\n",
    "# print(X[0])\n",
    "# X_train, X_test = np.asarray(X_train), np.asarray(X_test)\n",
    "# Y_train, Y_test = np.asarray(Y_train), np.asarray(Y_test)\n",
    "Y = np.asarray(Y)\n",
    "# Y = Y.transpose()\n",
    "# print(X.shape, np.size(X))\n",
    "# print(Y.shape, np.size(Y))\n",
    "\n",
    "''' Using SVM to classify'''\n",
    "# Declaring variables for scores\n",
    "t_precision = 0\n",
    "t_accracy = 0\n",
    "t_recall = 0\n",
    "t_f1_score = 0\n",
    "clf = svm.SVC()\n",
    "KFold(n_splits = 10, random_state = None, shuffle = False)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    clf.fit(X_train, Y_train)\n",
    "    t_pred = clf.predict(X_test)\n",
    "    mod_accracy = accuracy_score(Y_test, t_pred)\n",
    "    t_accracy += mod_accracy\n",
    "    pr = precision_score(Y_test, t_pred, average='weighted')\n",
    "    re = recall_score(Y_test, t_pred, average='weighted')\n",
    "    f1_s = f1_score(Y_test, t_pred, average='weighted')\n",
    "    t_precision += pr\n",
    "    t_recall += re\n",
    "    t_f1_score += f1_s\n",
    "    t_accracy += mod_accracy\n",
    "\n",
    "print('\\n\\nModel Accuracy is: ', (mod_accracy/10))\n",
    "print('\\n\\nPrecision Score is: ', (t_precision/10))\n",
    "print('\\n\\nRecall Score is: ', (t_recall/10))\n",
    "print('\\n\\nF1-Score is: ', (t_f1_score/10))\n",
    "save_model = pickle.dumps(clf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
