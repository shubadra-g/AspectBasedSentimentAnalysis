{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3602, 3614)\n",
      "(3602, 3613)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load data set'''\n",
    "df1 = pd.read_csv(\"weightxy_data1.csv\")\n",
    "print(df1.shape)\n",
    "print(df1.iloc[:,:-1].shape)\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df1.iloc[:,:-1], df1['class_'], test_size = 0.3, random_state =42)\n",
    "# svm_clf = svm.SVC()\n",
    "svm_clf = svm.LinearSVC()\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Cross validation'''\n",
    "scores = cross_validate(svm_clf, X_train, Y_train, cv = 10)\n",
    "\n",
    "print(scores.keys())\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([ 0.64356565,  0.56521797,  0.67794037,  0.45257592,  0.47691512,\n",
      "        0.45390868,  0.59017301,  0.69643974,  0.51861835,  0.57461143]), 'score_time': array([ 0.02650523,  0.00540972,  0.00739384,  0.00300789,  0.00200486,\n",
      "        0.00435448,  0.02276063,  0.00603557,  0.0070045 ,  0.00501513]), 'test_score': array([ 0.73622047,  0.71146245,  0.73517787,  0.6984127 ,  0.72619048,\n",
      "        0.74603175,  0.72222222,  0.68525896,  0.74501992,  0.73306773]), 'train_score': array([ 0.98235554,  0.98280423,  0.98104056,  0.9845747 ,  0.98369326,\n",
      "        0.98413398,  0.98148964,  0.98325991,  0.98370044,  0.98105727])}\n",
      "trained the svm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores)\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "'''Train the svm clf'''\n",
    "svm_clf.fit(X_train, Y_train)\n",
    "print('trained the svm')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Predit and get the accuracy of the model'''\n",
    "t_pred = svm_clf.predict(X_test)\n",
    "mod_accracy = accuracy_score(Y_test, t_pred)\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Scores for SVM: ***\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.65      0.63       234\n",
      "          1       0.55      0.52      0.54       190\n",
      "         -1       0.86      0.86      0.86       657\n",
      "\n",
      "avg / total       0.75      0.75      0.75      1081\n",
      "\n",
      "\n",
      "\n",
      "Model Accuracy is: 75.2081%\n",
      "\n",
      "\n",
      "Precision Score is: 75.2213%\n",
      "\n",
      "\n",
      "Recall Score is: 75.2081%\n",
      "\n",
      "\n",
      "F1-Score is: 75.1946%\n"
     ]
    }
   ],
   "source": [
    "'''calculate the precision recall and stuff'''\n",
    "\n",
    "t_precision = precision_score(Y_test, t_pred, average='weighted')\n",
    "t_recall = recall_score(Y_test, t_pred, average='weighted')\n",
    "t_f1_score = f1_score(Y_test, t_pred, average='weighted')\n",
    "\n",
    "print('*** Scores for SVM: ***')\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(Y_test, t_pred, target_names=target_names))\n",
    "\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "print('\\n\\nPrecision Score is: {:.4%}' .format(t_precision))\n",
    "print('\\n\\nRecall Score is: {:.4%}' .format(t_recall))\n",
    "print('\\n\\nF1-Score is: {:.4%}' .format(t_f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
