{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fried\n",
      "(3602, 3612)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Apr  6 16:09:25 2018\n",
    "\n",
    "@author: Shubadra\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "find = lambda searchList, elem: [[i for i, x in enumerate(searchList) if x == e] for e in elem]\n",
    "fname = 'project_2_train/' + 'data 2_train.csv'\n",
    "# fname = 'project_2_train/' + 'data 2_train.csv'\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_set = {'i', 'shan', 'just', 'how', 'each', 'out', 'themselves', 'their', 'before', 'were', 'very', 'as', 'further', 'his', 'a', 'once', 'youve', 'y', 'is', 'shouldve', 'youll', 'on', 'd', 'm', 'under', 'haven', 'which', 'only', 'them', 'was', 'by', 'needn', 'whom', 'that', 'when', 's', 'isn', 'its', 'no', 'wasn', 'in', 'we', 'theirs', 'those', 'this', 'having', 'and', 'ain', 'most', 'up', 'off', 'being', 'aren', 'shouldn', 'ourselves', 'from', 'down', 'herself', 'her', 'you', 'are', 'its', 'who', 'the', 'here', 'where', 'your', 'youd', 'she', 'didn', 'weren', 'about', 'has', 'our', 'an', 'yourselves', 'or', 'hasn', 'again', 'while', 'does', 'him', 'shes', 'above', 'below', 'itself', 'to', 'through', 'will', 'couldn', 'hers', 'they', 'doing', 'because', 'he', 'what', 'such', 'youre', 'nor', 'too', 'should', 'ours', 'then', 'himself', 'all', 'of', 'mightn', 'between', 'now', 'against', 'some', 'with', 'until', 'am', 'other', 'at', 'can', 'over', 'mustn', 'wouldn', 'do', 'for', 'after', 'hadn', 'me', 'been', 'same', 'doesn', 'my', 'these', 'll', 'did', 'had', 'it', 'so', 'ma', 'during', 'than', 'o', 'yourself', 'own', 'have', 're', 've', 'be', 'why', 't', 'there', 'more', 'won', 'yours', 'few', 'into', 'thatll', 'any', 'myself', 'both', 'don', 'if'}\n",
    "# print(stopwords_set)\n",
    "# stopwords_set.remove('but')\n",
    "# stopwords_set.remove('not')\n",
    "\n",
    "sentence_weights = []\n",
    "X, Y, X1 = [], [], []\n",
    "\n",
    "f = open(fname, 'r')\n",
    "for i, line in enumerate(f):\n",
    "    if i != 0:\n",
    "        ''' Splitting the columns based on comma - since it is csv'''\n",
    "        columns = line.split(',')\n",
    "\n",
    "        ''' The comma in the actual sentence was represented as [commma] because of csv format, replace that '''\n",
    "        columns[1] = columns[1].replace('[comma]', ',')\n",
    "        columns[2] = columns[2].replace('[comma]', ',')\n",
    "\n",
    "        ''' Some aspect terms are mic in the sentence where computer/mic is present '''\n",
    "        columns[1] = columns[1].replace('-', ' ')\n",
    "        columns[2] = columns[2].replace('-', ' ')\n",
    "        columns[1] = columns[1].replace('/', ' ')\n",
    "        columns[2] = columns[2].replace('/', ' ')\n",
    "\n",
    "        ''' NOt used anymore'''\n",
    "        tokenizer = RegexpTokenizer(r'\\w+') # doesn't work if the special char is in the token\n",
    "        # columns[2] = tokenizer.tokenize(columns[2].lower())\n",
    "\n",
    "        '''Tokenize the words'''\n",
    "        columns[1] = word_tokenize(columns[1])\n",
    "        columns[2] = word_tokenize(columns[2])\n",
    "\n",
    "        for j, elem in enumerate(columns[1]):\n",
    "            '''Remove special characters'''\n",
    "            columns[1][j] = re.sub('[^0-9a-zA-Z]+', '', elem).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[1][j] == 'nt':\n",
    "                columns[1][j] = 'not'\n",
    "\n",
    "        for j, elem in enumerate(columns[2]):\n",
    "            '''Remove special characters'''\n",
    "            columns[2][j] = re.sub('[^0-9a-zA-Z]+', '', elem).lower()\n",
    "            ''' didn't is tokenized as did n't hence following code '''\n",
    "            if columns[2][j] == 'nt':\n",
    "                columns[2][j] = 'not'\n",
    "\n",
    "        '''Remove empty string tokens'''\n",
    "        columns[1] = [x.strip() for x in columns[1] if x.strip() != '']\n",
    "        columns[2] = [x.strip() for x in columns[2] if x.strip() != '']\n",
    "\n",
    "        ''' Remove stop words '''\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        columns[1] = [word for word in columns[1] if word not in stopwords_set]\n",
    "        columns[2] = [word for word in columns[2] if word not in stopwords_set]\n",
    "\n",
    "        columns[1] = ' '.join(columns[1])\n",
    "        columns[2] = ' '.join(columns[2])\n",
    "\n",
    "        ''' The aspect term location given is not proper - hence extracting the location by ourselves '''\n",
    "        columns[3] = []\n",
    "        ''' Finds the location of the aspect term in the sentence '''\n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "            ''' since some aspect terms are not surrounded by spaces, hence below code\n",
    "            NOT USED ANYMORE\n",
    "            '''\n",
    "            if m.start() != 0 and columns[1][m.start() - 1] != ' ':\n",
    "                # columns[1] = columns[1][:m.start()] + ' ' + columns[1][m.start():]\n",
    "                pass\n",
    "\n",
    "        ''' Because of adding space in above code, the position is messed up, hence redoing it. '''\n",
    "        columns[3] = []\n",
    "\n",
    "        for m in re.finditer(columns[2], columns[1]):\n",
    "            columns[3].append([m.start(), m.end()])\n",
    "\n",
    "        ''' Tokenizing the words again '''\n",
    "        columns[1] = columns[1].split(' ')\n",
    "        columns[2] = columns[2].split(' ')\n",
    "        ''' New column to specify the aspect term location in the list '''\n",
    "        columns.append([])\n",
    "        # columns[5] = []\n",
    "        ''' for multiple positions of aspect term '''\n",
    "        for elem in columns[3]:\n",
    "            if elem[0] == 0: #start index of the 1st occurance of the aspect term\n",
    "                columns[5].append([0])\n",
    "            else:\n",
    "                temp_len = elem[0] # assign the start position of the aspect term\n",
    "                for j, tokens in enumerate(columns[1]):\n",
    "                    temp_len -= (len(tokens) + 1) # Counting the words till the position - +1 for blank space\n",
    "                    if temp_len == 0: # Reached the aspect word\n",
    "                        columns[5].append([j+1])\n",
    "                        break\n",
    "            for k in range(len(columns[2])-1): # if multiple words in the aspect term - tag the following words in the sentence\n",
    "                columns[5][-1].append(columns[5][-1][-1]+1)\n",
    "\n",
    "        ''' if not found the aspect term location in the list then do '''\n",
    "        if len(columns[5]) < 1:\n",
    "            continue\n",
    "\n",
    "            # pass\n",
    "\n",
    "        word_wt_list = [[elem, 0] for elem in columns[1]]\n",
    "        ''' Assigning weights to every word based on the distance from the aspect term '''\n",
    "        for j, elem in enumerate(columns[5]):\n",
    "            for k, word in enumerate(columns[1]):\n",
    "                if k < elem[0]: # Word left to the aspect term\n",
    "                    dist = abs(elem[0] - k)\n",
    "                    if word_wt_list[k][1] < 1/dist:\n",
    "                        word_wt_list[k][1] = 1/dist\n",
    "                elif k > elem[-1]: # word right to the aspect term : Aspect term can have multiple words\n",
    "                    dist = abs(k - elem[-1])\n",
    "                    if word_wt_list[k][1] < 1/dist:\n",
    "                        word_wt_list[k][1] = 1/dist\n",
    "            for aspect_word_loc in elem:\n",
    "                word_wt_list[aspect_word_loc][1] = 1.5\n",
    "\n",
    "        ''' For duplicate words - if it's aspect term - add the weights, if not, then take the weight which is greater, make the other weight as zero '''\n",
    "        for j, word_1 in enumerate(word_wt_list):\n",
    "            for k, word_2 in enumerate(word_wt_list[j+1:]):\n",
    "                if word_1[0] == word_2[0]:\n",
    "                    if word_1[1] == 1.5 and word_2[1] == 1.5:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_1[1] += 1.5\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] < word_2[1]:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_1[1] = word_2[1]\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] > word_2[1]:\n",
    "                        # print(line)\n",
    "                        # print(word_wt_list)\n",
    "                        word_2[1] = 0\n",
    "                    elif word_1[1] == word_2[1]:\n",
    "                        word_2[1] = 0\n",
    "\n",
    "\n",
    "        ''' For removing duplicate words - they have weight zero now'''\n",
    "        word_wt_list = [elem for elem in word_wt_list if elem[1] != 0]\n",
    "\n",
    "        if len(columns[2]) > 1 and len(columns[5]) > 1:\n",
    "            # print(line)\n",
    "            # print(columns)\n",
    "            # print(word_wt_list)\n",
    "            pass\n",
    "\n",
    "        if columns[0] == '2911_0':\n",
    "            # print(line)\n",
    "            # print(word_wt_list)\n",
    "            pass\n",
    "        if i == 100:\n",
    "            # break\n",
    "            pass\n",
    "\n",
    "        '''Removing new line character from columns[4] -  the polarity column'''\n",
    "        columns[4] = columns[4].rstrip('\\n')\n",
    "        X.append(columns[1])\n",
    "        X1.append(' '.join(columns[1]))\n",
    "\n",
    "        Y.append(columns[4])\n",
    "        sentence_weights.append(word_wt_list)\n",
    "\n",
    "\n",
    "'''Building vocabulary and counting word occurances'''\n",
    "X_reduced = reduce(lambda x1, x2: x1 + x2, X)\n",
    "vocab = list(set(X_reduced))\n",
    "print(vocab[0])\n",
    "weight_v = np.zeros_like(vocab, dtype = np.float_)\n",
    "weight_x = []\n",
    "# pf = open('weightxy_data1.csv', 'w')\n",
    "pf = open('weightxy_data2.csv', 'w')\n",
    "\n",
    "pf.write(','.join(vocab))\n",
    "pf.write(',class_')\n",
    "pf.write('\\n')\n",
    "for i, sentence in enumerate(sentence_weights):\n",
    "    for word in sentence:\n",
    "        v_index = vocab.index(word[0])\n",
    "        weight_v[v_index] = word[1]\n",
    "\n",
    "    weight_x.append(weight_v)\n",
    "    if len(sentence) != weight_v[np.where(weight_v > 0)].shape[0]:\n",
    "        print(sentence)\n",
    "        print(len(sentence))\n",
    "        print(weight_v[np.where(weight_v > 0)])\n",
    "\n",
    "    ''' code to write the data to file '''\n",
    "    temp = [str(j) for j in weight_v.tolist()]\n",
    "    pf.write(','.join(temp))\n",
    "    pf.write(','+str(Y[i]))\n",
    "    pf.write('\\n')\n",
    "    weight_v = np.zeros_like(weight_v)\n",
    "\n",
    "pf.close()\n",
    "weight_x = np.array(weight_x)\n",
    "\n",
    "print(weight_x.shape)\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "# weight_X2 = np.reshape(weight_X, (len(Y) ,weight_X.shape[0]))\n",
    "# print(weight_X2.shape)\n",
    "# tokens = f.split(',') # columns\n",
    "# remove punctuations\n",
    "# table = str.maketrans('', '', string.punctuation)\n",
    "# print(table)\n",
    "# tokens = [w.translate(table) for w in tokens]\n",
    "# # remove tokens that are not alphbetic\n",
    "# tokens = [word for word in tokens if word.isalpha()]\n",
    "#\n",
    "# # remove stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tokens = [w for w in tokens if not w in stop_words]\n",
    "# # filter out short tokens\n",
    "# tokens = [word for word in tokens if len(word) > 1]\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "(3602, 3613)\n",
      "(3602, 3612)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "print('success')\n",
    "'hello world'\n",
    "\n",
    "'''Load data set'''\n",
    "df1 = pd.read_csv(\"weightxy_data2.csv\")\n",
    "print(df1.shape)\n",
    "print(df1.iloc[:,:-1].shape)\n",
    "'hello world'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "df1 (3602, 3613)\n",
      "processed df1 (3602, 3503)\n"
     ]
    }
   ],
   "source": [
    "# print(df1['Unnamed: 0'])\n",
    "\n",
    "count = 0\n",
    "# processed_df1 = df1[['class_']].copy()\n",
    "processed_df1 = pd.DataFrame()\n",
    "temp_one_AT = pd.DataFrame()\n",
    "temp_df2 = pd.DataFrame()\n",
    "\n",
    "for column in df1:\n",
    "#     print(column)\n",
    "    temp_array = np.array(df1[column].values)\n",
    "#     print(np.where(temp_array > 0)[0].shape)\n",
    "    if np.where(temp_array > 0)[0].shape[0] <= 1:\n",
    "        if pd.DataFrame.max(df1[column]) > 1:\n",
    "            temp_one_AT[column] = df1[[column]].copy()\n",
    "            count = count + 1\n",
    "            pass\n",
    "        else:\n",
    "            temp_df2[column] = df1[[column]].copy()\n",
    "            processed_df1[column] = df1[[column]].copy()\n",
    "#             count = count + 1\n",
    "#             print(column)\n",
    "    else: \n",
    "        processed_df1[column] = df1[[column]].copy()\n",
    "        pass\n",
    "\n",
    "processed_df1['temp_1'] = temp_one_AT.sum(axis=1)\n",
    "# processed_df1[temp_2] = temp_df2.sum(axis=1)\n",
    "    \n",
    "print(count)\n",
    "print('df1', df1.shape)\n",
    "print('processed df1', processed_df1.shape)\n",
    "#     if pd.DataFrame.max(df1[column]) == 0:\n",
    "#         print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9cbef6d0b42a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown backend: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# Framework\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework_lib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIndexedSlices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_tensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_tensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparseTensorValue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "X, Y = df1.drop(['class_'], axis = 1), df1['class_']\n",
    "# X, Y = processed_df1.drop(['class_'], axis = 1), processed_df1['class_']\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "# print(encoder.classes_)\n",
    "\n",
    "encoder.fit(Y)\n",
    "print(encoder.classes_)\n",
    "print(Y[0])\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# print(encoded_Y.classes)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# print(dummy_y)\n",
    "print('Working on model...')\n",
    "\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# create model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(8, input_dim=3612, activation='relu'))\n",
    "# model.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Start\n",
    "''' Trying out CV'''\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "# results = cross_val_score(model, X, Y, cv=kfold)\n",
    "# y_pred_ma = cross_val_predict(model, X, Y, cv=kfold)\n",
    "y_pred_ma = cross_validate(model, X, Y, cv=kfold)\n",
    "# end\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, dummy_y, test_size = 0.2, random_state = 42)\n",
    "# print(np.arange(Y_test).reshape())\n",
    "\n",
    "# y_pred = np.zeros_like(Y_test)\n",
    "\n",
    "# model.fit(X_train, Y_train, epochs = 10, batch_size = 10, shuffle = False)\n",
    "\n",
    "# pred = model.predict(X_test)\n",
    "# print(pred)\n",
    "    \n",
    "\n",
    "\n",
    "# for pred_index, pred_val in enumerate(y_pred):\n",
    "\n",
    "#     if y_pred[pred_index][0]:\n",
    "#         y_pred_label[pred_index] = -1\n",
    "#     elif y_pred[pred_index][1]:\n",
    "#         y_pred_label[pred_index] = 0    \n",
    "#     else:\n",
    "#         y_pred_label[pred_index] = 1    \n",
    "\n",
    "        \n",
    "    \n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# mod_accracy = accuracy_score(Y_test, y_pred)\n",
    "# print(mod_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_ma = cross_val_predict(model, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model Accuracy is: 74.4586%\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.64      0.63       780\n",
      "          1       0.53      0.53      0.53       632\n",
      "         -1       0.86      0.85      0.85      2190\n",
      "\n",
      "avg / total       0.75      0.74      0.75      3602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod_accracy = accuracy_score(y_pred_ma, Y)\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(y_pred_ma, Y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
