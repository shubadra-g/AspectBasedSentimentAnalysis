{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2313, 2969)\n",
      "(2313, 2968)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load data set'''\n",
    "df1 = pd.read_csv(\"weightxy_data1.csv\")\n",
    "print(df1.shape)\n",
    "print(df1.iloc[:,:-1].shape)\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df1.iloc[:,:-1], df1['class_'], test_size = 0.3, random_state =42)\n",
    "# svm_clf = svm.SVC()\n",
    "svm_clf = svm.LinearSVC()\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Cross validation'''\n",
    "scores = cross_validate(svm_clf, X_train, Y_train, cv = 10)\n",
    "\n",
    "print(scores.keys())\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([ 0.28669977,  0.45873642,  0.36379838,  0.33316255,  0.38324523,\n",
      "        0.32109165,  0.31468558,  0.36875939,  0.31052065,  0.32900763]), 'score_time': array([ 0.01853132,  0.00466037,  0.00246549,  0.00200605,  0.00476217,\n",
      "        0.        ,  0.        ,  0.01562786,  0.01559281,  0.        ]), 'test_score': array([ 0.73619632,  0.68711656,  0.65432099,  0.7037037 ,  0.7037037 ,\n",
      "        0.66049383,  0.67901235,  0.71604938,  0.63354037,  0.775     ]), 'train_score': array([ 0.99313187,  0.99313187,  0.99313658,  0.99313658,  0.99313658,\n",
      "        0.9917639 ,  0.9917639 ,  0.99107756,  0.99176955,  0.99246059])}\n",
      "trained the svm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores)\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "'''Train the svm clf'''\n",
    "svm_clf.fit(X_train, Y_train)\n",
    "print('trained the svm')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Predit and get the accuracy of the model'''\n",
    "t_pred = svm_clf.predict(X_test)\n",
    "mod_accracy = accuracy_score(Y_test, t_pred)\n",
    "print('success')\n",
    "'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Scores for SVM: ***\n",
      "Pr, Re scores wrt each class\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.74      0.73       264\n",
      "          1       0.48      0.45      0.46       131\n",
      "         -1       0.79      0.79      0.79       299\n",
      "\n",
      "avg / total       0.70      0.71      0.71       694\n",
      "\n",
      "\n",
      "\n",
      "Model Accuracy is: 70.7493%\n",
      "\n",
      "\n",
      "Precision Score is: 70.4716%\n",
      "\n",
      "\n",
      "Recall Score is: 70.7493%\n",
      "\n",
      "\n",
      "F1-Score is: 70.5955%\n"
     ]
    }
   ],
   "source": [
    "'''calculate the precision recall and stuff'''\n",
    "\n",
    "t_precision = precision_score(Y_test, t_pred, average='weighted')\n",
    "t_recall = recall_score(Y_test, t_pred, average='weighted')\n",
    "t_f1_score = f1_score(Y_test, t_pred, average='weighted')\n",
    "\n",
    "print('*** Scores for SVM: ***')\n",
    "target_names = ['0', '1', '-1']\n",
    "print('Pr, Re scores wrt each class')\n",
    "print(classification_report(Y_test, t_pred, target_names=target_names))\n",
    "\n",
    "print('\\n\\nModel Accuracy is: {:.4%}' .format(mod_accracy))\n",
    "print('\\n\\nPrecision Score is: {:.4%}' .format(t_precision))\n",
    "print('\\n\\nRecall Score is: {:.4%}' .format(t_recall))\n",
    "print('\\n\\nF1-Score is: {:.4%}' .format(t_f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
